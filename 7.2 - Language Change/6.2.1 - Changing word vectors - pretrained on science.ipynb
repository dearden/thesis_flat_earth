{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import sys\n",
    "sys.path.insert(1, \"../\")\n",
    "sys.path.insert(1, \"../utilities\")\n",
    "\n",
    "from language_change_methods.utility_functions import tokenise, get_time_windows, get_data_windows, count_tokens\n",
    "from language_change_methods.cross_entropy import single_CE_run\n",
    "\n",
    "from helpers import load_posts, load_toks, load_pos, load_ent\n",
    "\n",
    "GRAPH_DIR = \"./Graphs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_db(db_fp, query):\n",
    "    conn = sqlite3.connect(db_fp)\n",
    "    comments = pd.read_sql_query(query, conn, index_col=\"uid\", parse_dates={\"time\": \"%Y/%m/%d %H:%M:%S\"})\n",
    "    comments.index = comments.index.astype(str)\n",
    "    comments.sort_values(\"time\", inplace=True)\n",
    "    return comments\n",
    "\n",
    "\n",
    "def get_align_toks(tok_fp, posts):\n",
    "    # Get the corresponding tokens\n",
    "    toks = {x[0]: x[1] for x in load_toks(tok_fp)}\n",
    "    toks = pd.Series(toks)\n",
    "    toks = toks[toks.index.isin(posts.index)]\n",
    "\n",
    "    # Remove the posts that don't have tokens\n",
    "    posts = posts[posts.index.isin(toks.index)]\n",
    "    # Align the ordering of forum posts and toks\n",
    "    toks = toks.loc[posts.index]\n",
    "\n",
    "    return toks, posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import TFES_FP as FORUM_DB_FP, TFES_TOK_FP as FORUM_TOKS_FP\n",
    "from helpers import flat_earth_boards, off_topic_boards as other_boards\n",
    "\n",
    "sql_get_forum =\"\"\"\n",
    "                SELECT p.uid AS uid, p.time AS time, p.user AS poster_id, b.uid AS board_id\n",
    "                FROM posts as p\n",
    "                INNER JOIN topics as t\n",
    "                ON t.uid = p.topic\n",
    "                INNER JOIN boards as b\n",
    "                ON b.uid = t.board;\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gets all flat earth posts\n",
    "forum_posts = read_db(FORUM_DB_FP, sql_get_forum)\n",
    "\n",
    "forum_toks, forum_posts = get_align_toks(FORUM_TOKS_FP, forum_posts)\n",
    "\n",
    "fe_posts = forum_posts.query(\"board_id in @flat_earth_boards\")\n",
    "ot_posts = forum_posts.query(\"board_id in @other_boards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 555 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "from settings import SCIENCE_W2V_FP\n",
    "model = Word2Vec.load(SCIENCE_W2V_FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This model will be continually updated\n",
    "updated_model = deepcopy(model)\n",
    "\n",
    "time_models = dict()\n",
    "# Train a language model for various different portions of the forum.\n",
    "for w, w_posts in get_data_windows(fe_posts, 10000, 10000, time_column=\"time\"):\n",
    "    updated_model.build_vocab(forum_toks.loc[w_posts.index], update=True)\n",
    "    updated_model.train(forum_toks.loc[w_posts.index], total_examples=len(w_posts), epochs=5)\n",
    "    time_models[w] = deepcopy(updated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time_models_not_gradual = dict()\n",
    "# Train a language model for various different portions of the forum.\n",
    "for w, w_posts in get_data_windows(fe_posts, 10000, 10000, time_column=\"time\"):\n",
    "    curr_model = deepcopy(model)\n",
    "    curr_model.build_vocab(forum_toks.loc[w_posts.index], update=True)\n",
    "    curr_model.train(forum_toks.loc[w_posts.index], total_examples=len(w_posts), epochs=5)\n",
    "    time_models_not_gradual[w] = deepcopy(curr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_vector_change import neighbours_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['round', 'rule', 'globe', 'conspiracy', 'pointing', 'consensus', 'religious', 'liberal', 'conservative', 'map', 'conservatives', 'christian']\n",
      "2015-12-30 23:54:31\n",
      "['round', 'globe', 'fe', 'sphere', 'bot', 'repost', 'rule', 'shape', 'spherical', 'reasonable', 'map', 'conspiracy']\n",
      "2017-01-16 01:34:55\n",
      "['round', 'globe', 'fe', 'sphere', 'map', 'spherical', 'bot', 'shape', 'infinite', 'basis', 'curvature', 'rule']\n",
      "2017-11-09 16:57:19\n",
      "['round', 'globe', 'fe', 'spherical', 'shape', 'sphere', 'conspiracy', 'curvature', 'infinite', 'burden', 'consensus', 'male']\n",
      "2018-04-20 09:03:11\n",
      "['round', 'globe', 'fe', 'spherical', 'sphere', 'curved', 'shape', 'bot', 'fet', 'infinite', 'basis', 'map']\n",
      "2018-10-07 05:55:37\n",
      "['round', 'globe', 'fe', 'spherical', 'sphere', 'curved', 'infinite', 'shape', 'fet', 'bot', 'curvature', 'male']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"flat\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['round', 'rule', 'globe', 'conspiracy', 'pointing', 'consensus', 'religious', 'liberal', 'conservative', 'scientific', 'map', 'climate']\n",
      "2015-12-30 23:54:31\n",
      "['round', 'globe', 'conspiracy', 'rule', 'fe', 'false', 'repost', 'sphere', 'map', 'wrong', 'reasonable', 'true']\n",
      "2017-01-16 01:34:55\n",
      "['round', 'globe', 'fe', 'map', 'rule', 'conspiracy', 'hypothesis', 'consensus', 'true', 'scientific', 'warming', 'sphere']\n",
      "2017-11-09 16:57:19\n",
      "['round', 'globe', 'fe', 'rule', 'conspiracy', 'scientific', 'climate', 'map', 'hypothesis', 'there', 'pointing', 'sphere']\n",
      "2018-04-20 09:03:11\n",
      "['round', 'globe', 'rule', 'map', 'fe', 'conspiracy', 'valid', 'scientific', 'warming', 'false', 'true', 'spherical']\n",
      "2018-10-07 05:55:37\n",
      "['globe', 'round', 'fe', 'sphere', 'map', 'rule', 'model', 'spherical', 'infinite', 'shape', 'conspiracy', 'climate']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"flat\", time_models_not_gradual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Changiest Words per window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from word_vector_change import get_changiest_words_per_window\n",
    "\n",
    "changiest_words_per_window = get_changiest_words_per_window(time_models, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_vector_change import print_changiest_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "fe                   even                 also                 globe                actually            \n",
      "earth                flat                 only                 re                   moon                \n",
      "n't                  not                  just                 very                 map                 \n",
      "why                  exactly              how                  love                 wiki                \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "also                 actually             still                even                 tom                 \n",
      "how                  only                 n't                  fe                   flat                \n",
      "not                  fet                  now                  perspective          just                \n",
      "why                  here                 flight               very                 earth               \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "also                 even                 n't                  still                actually            \n",
      "earth                now                  only                 2                    flat                \n",
      "how                  probably             why                  not                  very                \n",
      "well                 simply               again                do                   just                \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "n't                  even                 again                also                 still               \n",
      "actually             earth                just                 2                    now                 \n",
      "only                 exactly              flat                 not                  how                 \n",
      "never                globe                level                here                 1                   \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "even                 also                 only                 still                n't                 \n",
      "3                    just                 earth                not                  again               \n",
      "moon                 now                  flat                 matter               actually            \n",
      "how                  globe                why                  never                2                   \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print_changiest_over_time(changiest_words_per_window, time_models, 50, remove_punc=True, remove_func=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from word_vector_change import get_changiest_words_per_window\n",
    "\n",
    "changiest_words_per_window_2 = get_changiest_words_per_window(time_models_not_gradual, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "fe                   actually             also                 only                 globe               \n",
      "even                 re                   flat                 n't                  moon                \n",
      "still                quite                map                  exactly              now                 \n",
      "almost               tom                  not                  why                  love                \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "also                 actually             fe                   tom                  even                \n",
      "only                 n't                  very                 flat                 now                 \n",
      "well                 still                flight               perhaps              quite               \n",
      "globe                gps                  almost               re                   why                 \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "also                 even                 actually             only                 fe                  \n",
      "now                  flat                 still                flight               how                 \n",
      "n't                  never                re                   why                  2                   \n",
      "supposedly           very                 globe                just                 not                 \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "also                 n't                  actually             only                 still               \n",
      "even                 gps                  2                    fe                   flat                \n",
      "now                  again                globe                re                   already             \n",
      "just                 well                 horizon              how                  perhaps             \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "also                 even                 still                fe                   only                \n",
      "flat                 n't                  actually             just                 matter              \n",
      "earth                moon                 1                    well                 maybe               \n",
      "again                perhaps              globe                's                   very                \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print_changiest_over_time(changiest_words_per_window_2, time_models, 50, remove_punc=True, remove_func=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['model', 'evolution', 'hypothesis', 'fet', 'climate', 'theory', 'quantum', 'theories', 'science', 'submission', 'religion', 'discussion']\n",
      "2015-12-30 23:54:31\n",
      "['hypothesis', 'model', 'article', 'theories', 'scientific', 'science', 'relativity', 'evolution', 'predictions', 'arguments', 'gravity', 'logic']\n",
      "2017-01-16 01:34:55\n",
      "['fet', 'evolution', 'universe', 'theory', 'physics', 'theories', 'relativity', 'model', 'fe', 'climate', 'hypothesis', 'perspective']\n",
      "2017-11-09 16:57:19\n",
      "['universe', 'gravity', 'theory', 'model', 'evolution', 'particle', 'fe', 'relativity', 'principle', 'equation', 'physics', 'explanation']\n",
      "2018-04-20 09:03:11\n",
      "['fe', 'evolution', 'theory', 'scientific', 'flat', 'model', 'conspiracy', 'warming', 'belief', 'universe', 'science', 'hypothesis']\n",
      "2018-10-07 05:55:37\n",
      "['fe', 'model', 'quantum', 'theory', 'universe', 'evolution', 'science', 'scientific', 'fda', 'relativity', 'mechanism', 'explanation']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"fet\", time_models_not_gradual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_fe_code",
   "language": "python",
   "name": "thesis_fe_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
