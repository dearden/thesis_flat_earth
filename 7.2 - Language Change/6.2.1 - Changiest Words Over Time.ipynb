{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk import ngrams as make_ngrams\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "sys.path.insert(1, \"../\")\n",
    "sys.path.insert(1, \"../utilities\")\n",
    "\n",
    "from helpers import load_posts, load_toks, load_pos, get_top_n_toks\n",
    "from language_change_methods.vnc import VNC, plot_vnc\n",
    "from language_change_methods.utility_functions import get_data_windows, get_time_windows, basic_preprocessing\n",
    "from language_change_methods.features import get_tok_counts, function_words, combine_counts, make_feature_matrix\n",
    "\n",
    "# This method calculates cosine distance between two vectors.\n",
    "from scipy.spatial.distance import cosine as cosine_dist\n",
    "# This method simply inverts it to get similarity.\n",
    "cosine_sim = lambda x,y: 1 - cosine_dist(x,y)\n",
    "\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# suppress some deprecation warning..\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from settings import TFES_FP as DB_FP, TFES_TOK_FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_posts = load_posts(DB_FP)\n",
    "\n",
    "from helpers import flat_earth_boards, off_topic_boards as other_boards\n",
    "\n",
    "fe_posts = all_posts.query(\"board_id in @flat_earth_boards\")\n",
    "\n",
    "toks = {int(x[0]): x[1] for x in load_toks(TFES_TOK_FP)}\n",
    "toks = pd.Series(toks)\n",
    "toks = toks[toks.index.isin(fe_posts.index)]\n",
    "\n",
    "fe_posts = fe_posts.loc[toks.index]\n",
    "fe_posts.sort_values(\"time\", ascending=True)\n",
    "toks = toks.loc[fe_posts.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models Over Time\n",
    "\n",
    "In this section we will train the models on our data. First we'll look at two time periods - the first and second half of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_half = toks.iloc[:int(len(fe_posts)/2)]\n",
    "second_half = toks.iloc[int(len(fe_posts)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_1 = Word2Vec(first_half, size=300)\n",
    "model_2 = Word2Vec(second_half, size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_changey_words_with_models(model1, model2, n=100, k=1000, top_n=None):\n",
    "    nn_scores = []\n",
    "    \n",
    "    top_vocab = sorted(model1.wv.vocab.keys(), key=lambda x: model1.wv.vocab[x].count, reverse=True)[:top_n]\n",
    "    \n",
    "    vocab1 = model1.wv.vocab\n",
    "    vocab2 = model2.wv.vocab\n",
    "    # Loop through all the words in the vocab\n",
    "    for w in vocab1:\n",
    "        if (w not in function_words \n",
    "                and w in vocab1 \n",
    "                and w in vocab2 \n",
    "                and vocab1[w].count > n \n",
    "                and vocab2[w].count > n \n",
    "                and w in top_vocab):\n",
    "            neighbours1 = set([x[0] for x in model1.wv.most_similar(w, topn=k)])\n",
    "            neighbours2 = set([x[0] for x in model2.wv.most_similar(w, topn=k)])\n",
    "            nn_scores.append((len(neighbours1.intersection(neighbours2)), w))\n",
    "            \n",
    "    nn_scores_sorted = sorted(nn_scores)\n",
    "    return nn_scores_sorted\n",
    "\n",
    "\n",
    "\n",
    "def neighbors(query : str,\n",
    "              embs: np.ndarray,\n",
    "              vocab: list,\n",
    "              K : int = 3) -> list:\n",
    "    sims = np.dot(embs[vocab.index(query),],embs.T)\n",
    "    output = []\n",
    "    for sim_idx in sims.argsort()[::-1][1:(1+K)]:\n",
    "        if sims[sim_idx] > 0:\n",
    "            output.append(vocab[sim_idx])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def get_most_changey_words_with_vectors(vocab1, vocab2, vectors1, vectors2, n=20, k=1000):\n",
    "    nn_scores = []\n",
    "    # Loop through all the words in the vocab\n",
    "    for w in vocab1:\n",
    "        if w not in function_words and w in vocab1 and w in vocab2:\n",
    "            neighbours1 = set(neighbors(w, vectors1, vocab1, k))\n",
    "            neighbours2 = set(neighbors(w, vectors2, vocab2, k))\n",
    "            nn_scores.append((len(neighbours1.intersection(neighbours2)), w))\n",
    "            \n",
    "    nn_scores_sorted = sorted(nn_scores)\n",
    "    return nn_scores_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for changiest words from first to second half of corpus\n",
    "\n",
    "We have two subtley different methods for doing this.\n",
    "The first compares the models directly, while the second looks only at the top 10,000 words in the vocabulary.\n",
    "Using the entire models is more \"correct\", but with a small-ish corpus, the second method reduces the effects of low-occurance words and the output makes more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First by comparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranked_words_models = get_most_changey_words_with_models(model_1, model_2, n=10, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21, 'retarded'),\n",
       " (34, 'conundrum'),\n",
       " (37, 'fringe'),\n",
       " (37, 'venture'),\n",
       " (39, 'joules'),\n",
       " (39, 'whirlpool'),\n",
       " (40, 'flags'),\n",
       " (40, 'inflated'),\n",
       " (40, 'ir'),\n",
       " (41, 'corona'),\n",
       " (41, 'growth'),\n",
       " (42, \"'t\"),\n",
       " (43, 'exaggerated'),\n",
       " (43, 'mouse'),\n",
       " (43, 'spoof'),\n",
       " (44, 'bing'),\n",
       " (44, 'expanded'),\n",
       " (45, 'trend'),\n",
       " (48, 'appropriately'),\n",
       " (49, '----')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_words_models[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.wv.vocab[\"retarded\"].count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then by comparing the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_vocab_and_vectors(model, n=10000):\n",
    "    \"\"\"\n",
    "    Gets the top n words from the model's vocabulary and the vectors of these words.\n",
    "    \"\"\"\n",
    "    top_vocab = sorted(model.wv.vocab.keys(), key=lambda x: model.wv.vocab[x].count, reverse=True)[:n]\n",
    "    top_vectors = np.array([model.wv[t] for t in top_vocab])\n",
    "    return top_vocab, top_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab_1, vectors_1 = get_top_vocab_and_vectors(model_1)\n",
    "vocab_2, vectors_2 = get_top_vocab_and_vectors(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranked_words_vectors = get_most_changey_words_with_vectors(vocab_1, vocab_2, vectors_1, vectors_2, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(145, 'intuitively'),\n",
       " (155, 'spoof'),\n",
       " (167, 'retarded'),\n",
       " (178, 'whatâ\\x80\\x99s'),\n",
       " (206, 'endlessly'),\n",
       " (208, 'altogether'),\n",
       " (214, 'conundrum'),\n",
       " (215, 'sweeping'),\n",
       " (216, 'individually'),\n",
       " (230, 'beat'),\n",
       " (232, 'growth'),\n",
       " (233, 'arises'),\n",
       " (234, 'uncertain'),\n",
       " (235, 'bone'),\n",
       " (237, 'imho'),\n",
       " (237, 'replacing'),\n",
       " (238, 'qed'),\n",
       " (238, 'unbelievable'),\n",
       " (242, 'amazingly'),\n",
       " (242, 'nailed')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_words_vectors[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scientific',\n",
       " 'google',\n",
       " 'my',\n",
       " '3d',\n",
       " 'been',\n",
       " 'i',\n",
       " 'video',\n",
       " 'article',\n",
       " 'zetetic',\n",
       " 'your',\n",
       " ':',\n",
       " 'wiki',\n",
       " 'posted',\n",
       " 'link',\n",
       " 'rowbotham',\n",
       " 'provided',\n",
       " 'youtube',\n",
       " 'd',\n",
       " 'original',\n",
       " 'posting']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(\"3d\", vectors_1, vocab_1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sphere',\n",
       " 'projection',\n",
       " 'map',\n",
       " 'globe',\n",
       " 'd',\n",
       " 'circle',\n",
       " 'flat',\n",
       " 'coordinate',\n",
       " 'maps',\n",
       " 'plane',\n",
       " 'curved',\n",
       " 'lines',\n",
       " 'bing',\n",
       " 'scale',\n",
       " 'mercator',\n",
       " '3d',\n",
       " 'onto',\n",
       " '2d',\n",
       " 'spheroid',\n",
       " 'radius']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(\"3d\", vectors_2, vocab_2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for more gradual change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time_models = dict()\n",
    "# Train a language model for various different portions of the forum.\n",
    "for w, w_posts in get_data_windows(fe_posts, 10000, 10000):\n",
    "    time_models[w] = Word2Vec(toks.loc[w_posts.index], size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbours_over_time(search_term, time_models, top_n=10000):\n",
    "    for window, curr_model in time_models.items():\n",
    "        curr_vocab, curr_vectors = get_top_vocab_and_vectors(curr_model, top_n)\n",
    "        print(window)\n",
    "        if search_term in curr_vocab:\n",
    "            print(neighbors(search_term, curr_vectors, curr_vocab, 12))\n",
    "            \n",
    "            \n",
    "def neighbours_over_time_comma_delimited(query, time_models, top_n=10000):\n",
    "    for window, curr_model in time_models.items():\n",
    "        curr_vocab, curr_vectors = get_top_vocab_and_vectors(curr_model, top_n)\n",
    "        if query in curr_vocab:\n",
    "            print(window.strftime(\"%Y/%m/%d\"), end=\",\")\n",
    "            curr_neighbours = neighbors(query, curr_vectors, curr_vocab, 12)\n",
    "            print(\",\".join(curr_neighbours[:6]))\n",
    "            print(\"\", end=\",\")\n",
    "            print(\",\".join(curr_neighbours[6:]))\n",
    "        else:\n",
    "            print(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_fe_kw = pd.read_csv(\"../data/top-100-fe-keywords.csv\")\n",
    "t100_kw_list = list(t100_fe_kw[\"ngram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fe_kw = pd.read_csv(\"../data/all-fe-keywords.csv\")\n",
    "all_kw_list = list(all_fe_kw[\"ngram\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at some common FE related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['round', 'globe', 'shape', 'evidence', 'this', 'theory', 'i', 'fe', 'believe', 'model', 'map', 'what']\n",
      "2015-12-30 23:54:31\n",
      "['round', 'globe', 'shape', 'fe', 'evidence', 'sphere', 'spherical', 'map', 'proof', 'believe', 'wrong', 'me']\n",
      "2017-01-16 01:34:55\n",
      "['round', 'globe', 'fe', 'shape', 'map', 'model', 'evidence', 'believe', 'real', 'there', 'proof', 'true']\n",
      "2017-11-09 16:57:19\n",
      "['round', 'globe', 'fe', 'shape', 'believe', 'map', 'spherical', 'model', 'evidence', 'conspiracy', 'real', 'theory']\n",
      "2018-04-20 09:03:11\n",
      "['round', 'globe', 'fe', 'model', 'map', 'spherical', 'shape', 'sphere', 'moon', 'theory', 'believe', 'evidence']\n",
      "2018-10-07 05:55:37\n",
      "['round', 'globe', 'fe', 'spherical', 'map', 'model', 'shape', 'sphere', 'believe', 'i', 'moon', 'wrong']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"flat\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013/12/01,round,globe,shape,evidence,this,theory\n",
      ",i,fe,believe,model,map,what\n",
      "2015/12/30,round,globe,shape,fe,evidence,sphere\n",
      ",spherical,map,proof,believe,wrong,me\n",
      "2017/01/16,round,globe,fe,shape,map,model\n",
      ",evidence,believe,real,there,proof,true\n",
      "2017/11/09,round,globe,fe,shape,believe,map\n",
      ",spherical,model,evidence,conspiracy,real,theory\n",
      "2018/04/20,round,globe,fe,model,map,spherical\n",
      ",shape,sphere,moon,theory,believe,evidence\n",
      "2018/10/07,round,globe,fe,spherical,map,model\n",
      ",shape,sphere,believe,i,moon,wrong\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time_comma_delimited(\"flat\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['earthers', 'sun', 'horizon', 'model', 'world', 'map', 'surface', 'society', 'theory', 'pole', 'it', 'north']\n",
      "2015-12-30 23:54:31\n",
      "['earthers', 'horizon', 'sun', 'world', 'moon', 'map', 'earther', 'model', 'globe', 'surface', 'equator', 'pole']\n",
      "2017-01-16 01:34:55\n",
      "['world', 'earthers', 'horizon', 'map', 'sun', 'model', 'plane', 'moon', 'surface', 'earther', 'pole', 'theory']\n",
      "2017-11-09 16:57:19\n",
      "['earthers', 'world', 'sun', 'earther', 'horizon', 'moon', 'model', 'plane', 'map', 'surface', 'theory', 'fe']\n",
      "2018-04-20 09:03:11\n",
      "['sun', 'moon', 'surface', 'world', 'model', 'map', 'earthers', 'plane', 'horizon', 'light', 'sphere', 'earther']\n",
      "2018-10-07 05:55:37\n",
      "['surface', 'sun', 'plane', 'earthers', 'map', 'model', 'world', 'horizon', 'moon', 'earther', 'circle', 'maps']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"earth\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['round', 'globe', 'shape', 'theory', 'model', 'evidence', 'map', 'sun', 'earth', 'surface', 'believe', 'curvature']\n",
      "2015-12-30 23:54:31\n",
      "['globe', 'round', 'map', 'shape', 'model', 'sphere', 'earth', 'evidence', 'fe', 'moon', 'proof', 'theory']\n",
      "2017-01-16 01:34:55\n",
      "['round', 'globe', 'map', 'model', 'shape', 'fe', 'surface', 'earth', 'sphere', 'system', 'plane', 'based']\n",
      "2017-11-09 16:57:19\n",
      "['round', 'globe', 'shape', 'fe', 'map', 'model', 'sphere', 'spherical', 'conspiracy', 'earth', 'believe', 'plane']\n",
      "2018-04-20 09:03:11\n",
      "['round', 'globe', 'model', 'map', 'spherical', 'sphere', 'surface', 'shape', 'moon', 'fe', 'plane', 'theory']\n",
      "2018-10-07 05:55:37\n",
      "['globe', 'round', 'map', 'spherical', 'model', 'sphere', 'surface', 'plane', 'shape', 'fe', 'based', 'projection']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"globe\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['sun', 'flat', 'model', 'round', 'surface', 'horizon', 'globe', 'observer', 'map', 'distance', 'pole', 'north']\n",
      "2015-12-30 23:54:31\n",
      "['map', 'horizon', 'globe', 'sun', 'model', 'earthers', 'surface', 'earther', 'projection', 'object', 'moon', 'theory']\n",
      "2017-01-16 01:34:55\n",
      "['ice', 'map', 'model', 'surface', 'plane', 'wall', 'globe', 'horizon', 'object', 'force', 'flat', 'pole']\n",
      "2017-11-09 16:57:19\n",
      "['sun', 'miles', 'surface', 'horizon', 'equator', 'pole', 'north', 'moon', 'object', 'circle', 'plane', 'degrees']\n",
      "2018-04-20 09:03:11\n",
      "['surface', 'sun', 'plane', 'moon', 'sphere', 'light', 'distance', 'model', 'miles', '=', 'horizon', 'north']\n",
      "2018-10-07 05:55:37\n",
      "['sun', 'north', 'plane', 'pole', 'map', 'earth', 'circle', 'model', 'miles', 'equator', 'moon', 'south']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"disc\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['theory', 'sun', 'evidence', 'no', 'matter', 'acceleration', 'earth', 'light', 'model', 'aether', 'effect', 'exist']\n",
      "2015-12-30 23:54:31\n",
      "['flat', 'theory', 'round', 'evidence', 'fe', 'globe', 'sun', 'model', 'gravitation', 'true', 'shape', 'science']\n",
      "2017-01-16 01:34:55\n",
      "['perspective', 'fe', 'acceleration', 'theory', 'evidence', 'model', 'fet', 'anything', 'an', 'flat', 'tom', 'round']\n",
      "2017-11-09 16:57:19\n",
      "['model', 'perspective', 'acceleration', 'flat', 'force', 'round', 'exist', 'theory', 'shape', 'ua', 'gravitation', 'map']\n",
      "2018-04-20 09:03:11\n",
      "['theory', 'flat', 'evidence', 'fe', 'effect', 'gravity', 'perspective', 'force', 'round', 'refraction', 'coriolis', 'matter']\n",
      "2018-10-07 05:55:37\n",
      "['force', 'evidence', 'model', 'effect', 'fe', 'ua', 'theory', 'flat', 'acceleration', 'true', 'light', 'what']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"ua\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['earth', 'light', 'observer', 'north', 'surface', 'gravity', 'distance', 'moon', 'pole', 'object', '=', 'miles']\n",
      "2015-12-30 23:54:31\n",
      "['north', 'pole', 'wall', 'map', 'horizon', 'object', 'south', 'projection', 'earth', 'surface', 'celestial', 'equator']\n",
      "2017-01-16 01:34:55\n",
      "['wall', 'pole', 'north', 'south', 'force', 'be', 'map', 'acceleration', 'object', 'gravity', 'surface', 'plane']\n",
      "2017-11-09 16:57:19\n",
      "['wall', 'pole', 'has', 'south', 'object', 'there', 'north', 'force', 'light', 'shadow', 'gravity', '\"']\n",
      "2018-04-20 09:03:11\n",
      "['flat', 'pole', 'miles', 'ice', 'map', 'round', 'earth', 'north', 'wall', 'surface', 'model', 'globe']\n",
      "2018-10-07 05:55:37\n",
      "['wall', \"'\", 'evidence', 'model', 'pole', 'theory', 'has', '=', 'body', 'problem', 'earth', 'no']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"ice\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['light', 'earth', 'observer', 'distance', 'surface', 'north', 'gravity', 'pole', 'moon', 'miles', 'horizon', 'object']\n",
      "2015-12-30 23:54:31\n",
      "['map', 'wall', 'horizon', 'north', 'pole', 'globe', 'object', '\"', 'projection', 'surface', 'distance', 'south']\n",
      "2017-01-16 01:34:55\n",
      "['wall', 'pole', 'north', 'south', 'horizon', 'force', 'map', 'object', 'gravity', 'be', 'no', 'plane']\n",
      "2017-11-09 16:57:19\n",
      "['ice', 'pole', 'no', '\"', 'object', 'gravity', 'evidence', 'force', 'south', 'been', 'shadow', 'there']\n",
      "2018-04-20 09:03:11\n",
      "['surface', 'map', \"'\", 'plane', 'model', 'pole', 'globe', 'round', 'miles', 'earth', 'wall', 'sphere']\n",
      "2018-10-07 05:55:37\n",
      "['ice', \"'\", 'pole', 'evidence', 'there', 'no', 'has', 'model', 'flat', 'force', 'theory', 'effect']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"wall\", time_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at some of the top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['observer', '=', 'sun', '\\x94', 'distance', 'north', 'light', 'horizon', 'degrees', 'during', 'south', 'sky']\n",
      "2015-12-30 23:54:31\n",
      "['north', 'km', 'south', '°', 'degrees', 'miles', 'pole', '=', 'east', 'equator', 'angle', '̂°']\n",
      "2017-01-16 01:34:55\n",
      "['line', 'straight', 'north', 'light', 'southern', 'distance', 'south', 'pole', 'northern', 'speed', 'between', 'east']\n",
      "2017-11-09 16:57:19\n",
      "['miles', 'km', '=', 'north', 'equator', 'south', 'between', 'line', 'distance', 'lines', 'angle', 'straight']\n",
      "2018-04-20 09:03:11\n",
      "['north', 'pole', 'degrees', 'between', '=', 'miles', 'distance', 'lines', 'equator', 'distances', 'east', 'latitude']\n",
      "2018-10-07 05:55:37\n",
      "['south', 'equator', 'between', 'latitude', 'pole', 'degrees', 'line', 'km', 'east', 'distances', 'distance', 'longitude']\n",
      "-----------------------------------\n",
      "circumference\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['observer', 'distance', 'miles', 'horizon', 'north', 'light', 'surface', 'moon', 'pole', '\\x94', 'south', 'earth']\n",
      "2015-12-30 23:54:31\n",
      "['km', 'distance', 'horizon', 'north', 'sun', 'â°', 'equator', 'degrees', 'pole', 'south', '=', 'angle']\n",
      "2017-01-16 01:34:55\n",
      "['north', '=', 'degrees', 'ice', 'distance', 'south', 'feet', 'southern', 'surface', 'equator', 'pole', 'km']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'sun', 'km', 'south', 'north', 'equator', '=', 'pole', 'distance', 'surface', 'solar', 'angle']\n",
      "2018-04-20 09:03:11\n",
      "['miles', 'sun', 'surface', 'north', 'degrees', 'south', 'moon', 'rotation', 'distance', 'pole', 'km', 'globe']\n",
      "2018-10-07 05:55:37\n",
      "['=', 'm', 'km', 'degrees', 'diameter', 'surface', 'north', 'distance', 'feet', 'rotation', 'equator', 'center']\n",
      "-----------------------------------\n",
      "refraction\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['light', 'observer', 'miles', 'distance', 'horizon', 'north', 'moon', '\\x94', 'speed', '=', 'water', 'object']\n",
      "2015-12-30 23:54:31\n",
      "['horizon', 'light', 'object', 'angle', 'size', 'appear', 'gravity', 'surface', 'speed', 'force', 'pole', 'distance']\n",
      "2017-01-16 01:34:55\n",
      "['light', 'lines', 'gravity', 'acceleration', 'straight', 'sun', 'horizon', '\"', 'effect', 'line', 'moon', 'force']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'km', 'gravity', 'miles', 'object', 'acceleration', 'gravitation', 'force', 'shadow', 'mass', '2', 'light']\n",
      "2018-04-20 09:03:11\n",
      "['refraction', 'light', 'force', 'horizon', 'perspective', 'coriolis', 'gravity', 'eye', 'level', 'sagnac', '\"', 'surface']\n",
      "2018-10-07 05:55:37\n",
      "['effect', 'force', 'gravity', 'refraction', 'acceleration', 'horizon', 'object', 'mass', 'air', 'pressure', 'sun', 'atmosphere']\n",
      "-----------------------------------\n",
      "mirage\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['=', '\\x94', 'sun', 'distance', 'miles', 'north', 'light', 'horizon', 'moon', 'during', 'object', 'eclipse']\n",
      "2015-12-30 23:54:31\n",
      "['â°', 'km', '=', 'miles', 'object', 'x', '̂°', 'degrees', 'diameter', 'â', 'light', 'horizon']\n",
      "2017-01-16 01:34:55\n",
      "['you', \"'re\", 'miles', 'talking', 'people', 'he', 'tom', 'something', 'anything', 'anyone', \"'m\", 'we']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'miles', 'km', '0.0', '+', 'south', 'north', 'straight', 'gravitation', 'per', 'm', 'minutes']\n",
      "2018-04-20 09:03:11\n",
      "['light', 'force', 'eye', 'horizon', 'line', 'coriolis', 'refraction', 'level', 'sagnac', 'angle', '\"', 'perspective']\n",
      "2018-10-07 05:55:37\n",
      "['eye', 'light', 'm', 'effect', 'level', 'object', 'height', 'force', 'miles', 'feet', 'surface', 'sun']\n",
      "-----------------------------------\n",
      "mercator\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['\\x94', 'observer', 'theory', 'flat', 'globe', '1', 'round', '[', 'ether', 'map', 'model', 'earth']\n",
      "2015-12-30 23:54:31\n",
      "['globe', 'projection', '=', 'ice', 'km', 'wall', 'flat', 'surface', 'â°', '°', 'north', 'model']\n",
      "2017-01-16 01:34:55\n",
      "['round', 'map', 'model', 'fe', 'globe', 'based', 'earth', 'ice', 'bipolar', 'system', 'acceleration', 'theory']\n",
      "2017-11-09 16:57:19\n",
      "['km', 'degrees', '2', 'gravitation', 'gravity', '0.0', 'miles', 'force', 'celestial', '+', 'solar', 'surface']\n",
      "2018-04-20 09:03:11\n",
      "['=', 'globe', 'round', 'spherical', 'sphere', 'sagnac', 'based', 'surface', 'effect', 'system', 'theory', 'plane']\n",
      "2018-10-07 05:55:37\n",
      "['maps', 'based', 'system', 'model', 'plane', 'globe', 'earth', 'projection', 'spherical', '=', 'round', 'flat']\n",
      "-----------------------------------\n",
      "saros\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['\\x94', 'observer', 'sun', 'miles', 'during', 'north', 'eclipse', 'solar', '[', 'light', 'degrees', ']']\n",
      "2015-12-30 23:54:31\n",
      "2017-01-16 01:34:55\n",
      "['/', 'southern', 'm', 'solar', '1', 'celestial', 'northern', 'system', 'km', 'acceleration', 'x', 'eclipse']\n",
      "2017-11-09 16:57:19\n",
      "['miles', 'solar', 'years', 'south', 'hours', 'sun', 'km', 'southern', '=', 'moon', 'equator', 'pole']\n",
      "2018-04-20 09:03:11\n",
      "['system', '=', 'days', 'years', 'year', '24', 'day', 'based', 'model', 'times', 'hours', 'round']\n",
      "2018-10-07 05:55:37\n",
      "['years', 'solar', '/', 'm', 'x', 'eclipse', '[', '>', 'per', 'hours', 'degrees', '15']\n",
      "-----------------------------------\n",
      "santiago\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['=', 'observer', '\\x94', 'distance', 'north', 'degrees', 'km', '[', 'sun', 'light', 'during', ']']\n",
      "2015-12-30 23:54:31\n",
      "['°', 'â°', 'miles', '=', 'north', 'east', 'south', 'sydney', 'degrees', '2016', 'm', 'x']\n",
      "2017-01-16 01:34:55\n",
      "['miles', 'north', 'south', 'hours', 'west', 'degrees', 'm', 'feet', 'sydney', 'east', 'southern', 'km']\n",
      "2017-11-09 16:57:19\n",
      "['miles', 'km', '=', 'hours', 'south', 'minutes', 'feet', 'years', 'north', '10', 'm', '15']\n",
      "2018-04-20 09:03:11\n",
      "['degrees', 'miles', 'south', 'north', 'years', 'feet', 'm', 'hours', '|', '10', 'east', '6']\n",
      "2018-10-07 05:55:37\n",
      "['degrees', 'm', '=', 'feet', 'km', 'years', 'hours', 'per', 'south', 'east', '15', 'west']\n",
      "-----------------------------------\n",
      "equinox\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['sun', 'miles', 'distance', 'horizon', 'north', '\\x94', '=', 'light', 'moon', 'pole', 'sky', 'south']\n",
      "2015-12-30 23:54:31\n",
      "['north', 'km', '°', 'east', 'south', 'sun', 'pole', 'horizon', 'equator', 'miles', 'degrees', 'angle']\n",
      "2017-01-16 01:34:55\n",
      "['horizon', 'north', 'moon', 'equator', 'pole', 'miles', 'south', 'angle', 'southern', 'degrees', 'sunset', 'east']\n",
      "2017-11-09 16:57:19\n",
      "['sun', 'miles', 'equator', 'km', 'horizon', 'moon', 'north', 'sunset', 'distance', 'angle', 'south', '=']\n",
      "2018-04-20 09:03:11\n",
      "['moon', 'equinox', 'day', 'solar', 'year', 'degrees', 'equator', 'noon', 'observer', 'miles', 'angle', 'north']\n",
      "2018-10-07 05:55:37\n",
      "['moon', 'north', 'equator', 'day', 'east', 'degrees', 'observer', 'eclipse', 'west', 'south', 'sky', 'horizon']\n",
      "-----------------------------------\n",
      "clockwise\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['miles', 'observer', '\\x94', 'sun', 'north', 'distance', 'light', 'degrees', 'km', 'during', '[', 'solar']\n",
      "2015-12-30 23:54:31\n",
      "['north', 'â°', 'hemisphere', 'pole', 'southern', 'east', 'km', 'sun', 'northern', '°', 'degrees', 'miles']\n",
      "2017-01-16 01:34:55\n",
      "['south', 'pole', 'southern', 'east', 'northern', '=', 'equator', 'miles', 'west', 'hemisphere', 'sun', 'degrees']\n",
      "2017-11-09 16:57:19\n",
      "['=', 'miles', 'km', 'north', 'south', 'equator', 'm', 'pole', '+', '0.0', '2', '10']\n",
      "2018-04-20 09:03:11\n",
      "['south', 'degrees', 'north', 'east', 'west', 'pole', 'miles', 'feet', 'due', 'between', 'angle', '|']\n",
      "2018-10-07 05:55:37\n",
      "['north', 'miles', 'west', 'degrees', 'south', 'm', 'sun', 'pole', 'horizon', 'feet', 'km', '=']\n",
      "-----------------------------------\n",
      "trigonometry\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['=', '\\x94', 'sun', 'miles', 'distance', 'north', 'horizon', 'earth', '[', 'equator', 'light', 'km']\n",
      "2015-12-30 23:54:31\n",
      "['evidence', 'science', 'gravity', 'scientific', 'based', 'proof', 'model', 'projection', 'map', 'globe', 'fe', 'their']\n",
      "2017-01-16 01:34:55\n",
      "['moved', 'm', 'x', '1', '/', '+', 'flight', 'by', 'own', 'system', 'ms', \"'ve\"]\n",
      "2017-11-09 16:57:19\n",
      "['gravity', 'model', 'these', 'gravitation', 'lines', 'perspective', '0.0', 'your', 'science', 'his', 'evidence', 'theory']\n",
      "2018-04-20 09:03:11\n",
      "['|', 'south', '[', 'm', '/', 'degrees', 'distances', 'between', 'miles', 'north', '+', 'east']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-07 05:55:37\n",
      "['earth', 'map', 'plane', 'system', 'model', 'distances', 'surface', 'earthers', 'distance', 'circle', 'based', 'models']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for w in t100_kw_list[:10]:\n",
    "    print(w)\n",
    "    print(\"-----------------------------------\")\n",
    "    neighbours_over_time(w, time_models)\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for the changiest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changiest_words_per_window(time_models, top_n=10000):\n",
    "    out_dic = dict()\n",
    "    windows = list(time_models.keys())\n",
    "    for i in range(1, len(windows)):\n",
    "        model_1 = time_models[windows[i-1]]\n",
    "        model_2 = time_models[windows[i]]\n",
    "\n",
    "        vocab_1, vectors_1 = get_top_vocab_and_vectors(model_1, top_n)\n",
    "        vocab_2, vectors_2 = get_top_vocab_and_vectors(model_2, top_n)\n",
    "\n",
    "        out_dic[windows[i]] = get_most_changey_words_with_vectors(vocab_1, vocab_2, vectors_1, vectors_2, k=1000)\n",
    "\n",
    "    return out_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "changiest_words_per_window = get_changiest_words_per_window(time_models, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_lists = lambda x: list(itertools.chain.from_iterable(x))\n",
    "all_words = set(merge_lists([[cw[1] for cw in cws] for cws in changiest_words_per_window.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_all_windows(changiest_words_per_window):\n",
    "    words_in_each_window = [set([cw[1] for cw in cws]) for cws in changiest_words_per_window.values()]\n",
    "    words_in_all_windows = words_in_each_window[0].intersection(*words_in_each_window[1:])\n",
    "    return words_in_all_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_all_windows = get_words_in_all_windows(changiest_words_per_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected 266        cancer 272           respect 294          inclined 305         universally 311     \n",
      "f 326                philosophical 332    alt 336              peoples 343          pedantic 346        \n",
      "eric 351             invent 351           qualified 356        settled 356          explore 357         \n",
      "slightest 358        offers 359           eratosthenes 360     ludicrous 360        google 362          \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "notes 218            particularly 219     young 238            peoples 241          functions 261       \n",
      "flatness 271         discern 281          ** 282               sattelites 296       junk 303            \n",
      "3d 310               explaining 318       universally 322      seemingly 325        super 328           \n",
      "representative 329   powers 331           dangerous 336        moderate 341         ham 342             \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax 227         compute 279          dropped 302          needing 357          .. 369              \n",
      "neat 369             directed 371         flatness 380         expressed 382        remotely 382        \n",
      "functions 385        powers 385           trusting 391         instantly 394        navy 394            \n",
      "pin 396              peoples 398          -- 401               survey 402           alternatively 406   \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "drives 235           inclined 264         -- 274               practically 296      endless 301         \n",
      "flag 305             reveal 309           intuitive 313        stream 313           occurred 314        \n",
      "helped 320           fully 326            continually 330      shallow 337          photoshopped 338    \n",
      "trig 343             implied 345          ordinary 347         arenât 356        musk 364            \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "..... 173            empty 251            technically 279      letter 283           dangerous 284       \n",
      "drives 299           infrared 312         constantly 313       shifting 315         altered 320         \n",
      "beat 323             assess 325           seismic 337          steps 338            aid 341             \n",
      "convexity 341        jets 342             arenât 348        ?? 349               fits 351            \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in changey_words[:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected            cancer               respect              f                    alt                 \n",
      "slightest            eratosthenes         google               expected             fully               \n",
      "closed               arbitrary            experts              critical             special             \n",
      "ultimately           sunsets              chinese              particularly         popular             \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "particularly         3d                   explaining           super                dangerous           \n",
      "display              fully                giving               became               sunsets             \n",
      "becoming             precisely            deeper               pilots               latter              \n",
      "terrible             similarly            eratosthenes         agreed               heck                \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax             compute              powers               escape               itâs             \n",
      "rockets              rendering            3d                   incredibly           flag                \n",
      "giving               click                channel              particularly         key                 \n",
      "stay                 cgi                  super                holds                became              \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "inclined             practically          stream               fully                ordinary            \n",
      "immediately          holds                constantly           club                 typical             \n",
      "powerful             ruler                derive               instrument           fits                \n",
      "errors               required             roll                 a.                   blind               \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "constantly           beat                 seismic              fits                 assumed             \n",
      "consistently         gyroscope            closely              fully                eliminate           \n",
      "daily                itâ                everyday             stops                speaking            \n",
      "furthermore          finally              leaves               doctors              forever             \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "from language_change_methods.word_vector_change import print_changiest_over_time\n",
    "\n",
    "print_changiest_over_time(changiest_words_per_window, time_models, min_freq=30, remove_func=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['sun', 'light', 'miles', 'observer', '\\x94', 'north', 'distance', 'moon', 'degrees', 'solar', 'horizon', 'eclipse']\n",
      "2015-12-30 23:54:31\n",
      "['km', 'sun', '=', 'north', '°', 'miles', 'light', 'south', 'object', 'degrees', 'pole', 'hemisphere']\n",
      "2017-01-16 01:34:55\n",
      "['miles', 'light', 'speed', 'degrees', 'sun', 'due', 'feet', 'km', 'm', 'away', 'east', 'west']\n",
      "2017-11-09 16:57:19\n",
      "['tom', 'who', 'anyone', 'evidence', 'i', 'me', 'someone', 'rowbotham', 'been', 'thread', 'anything', 'what']\n",
      "2018-04-20 09:03:11\n",
      "['[', 'years', 'theory', ']', 'm', '|', '24', '10', 'been', 'days', 'miles', 'â\\x80']\n",
      "2018-10-07 05:55:37\n",
      "['relativity', '=', 'acceleration', 'sagnac', 'force', 'gravity', 'body', 'coriolis', 'system', 'caused', 'three', 'based']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"parallax\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "[\"'m\", 'am', 'post', 'thread', \"'ll\", 'have', 'want', 'had', 'read', 'i', 'question', 'here']\n",
      "2015-12-30 23:54:31\n",
      "['°', 'â°', '=', 'miles', '2016', 'degrees', 'â', 'years', '10', 'm', 'x', 'north']\n",
      "2017-01-16 01:34:55\n",
      "['m', 'miles', '1', '2', '/', 'x', 'km', '+', 'years', '%', 'hours', '*']\n",
      "2017-11-09 16:57:19\n",
      "['=', 'degrees', 'miles', 'years', 'km', '0.0', '+', 'south', 'feet', 'north', '10', 'm']\n",
      "2018-04-20 09:03:11\n",
      "['post', 'm', 'thread', 'years', '10', '[', '|', '*', ']', 'degrees', 'said', 'my']\n",
      "2018-10-07 05:55:37\n",
      "['been', 'miles', 'he', 'years', 'm', 'i', 'eye', 'wrong', 'able', 'sure', 'me', 'said']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"corrected\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "[\"n't\", \"'m\", 'sure', 'wrong', 'exist', 'talking', 'there', 'know', 'saying', ')', 'not', 'sun']\n",
      "2015-12-30 23:54:31\n",
      "['sun', 'appear', 'light', 'away', 'object', 'moon', 'distance', 'see', 'be', 'we', 'far', 'how']\n",
      "2017-01-16 01:34:55\n",
      "['lines', \"'\", 'light', 'north', 'gravity', 'ice', 'miles', 'acceleration', 'pole', 'straight', 'line', 'm']\n",
      "2017-11-09 16:57:19\n",
      "[\"'m\", '0.0', 'does', '2', 'am', 'degrees', '+', 'been', 'miles', 'km', 'question', '1']\n",
      "2018-04-20 09:03:11\n",
      "[\"'m\", 'are', \"'re\", 'been', 'saying', 'well', 'said', 'good', '!', 'he', 'was', 'sure']\n",
      "2018-10-07 05:55:37\n",
      "['m', 'miles', 'km', 'degrees', 'x', 'feet', 'pole', 'north', '>', 'eye', 'surface', 'south']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"technically\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['=', 'i', 'sure', 'am', \"'re\", '1', '[', 'do', ')', 'talking', ']', '2']\n",
      "2015-12-30 23:54:31\n",
      "['°', '=', 'km', 'â', '2016', 'south', 'north', '̂°', 'x', 'east', 'miles', 'size']\n",
      "2017-01-16 01:34:55\n",
      "[\"'ve\", 'he', 'me', 'i', 'did', 'questions', 'answer', 'am', 'tom', 'please', 'question', 'post']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'miles', 'km', '0.0', 'years', '+', 'm', 'my', 'thread', 'minutes', '10', 'topic']\n",
      "2018-04-20 09:03:11\n",
      "['who', \"'m\", 'me', 'thread', 'trying', 'wrong', 'evidence', 'nothing', 'question', 'sure', 'am', 'good']\n",
      "2018-10-07 05:55:37\n",
      "['me', 'science', 'my', 'sure', 'evidence', 'rowbotham', 'wrong', 'your', 'tom', 'who', 'i', 'does']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"particularly\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['sun', 'miles', 'observer', '\\x94', 'light', 'north', 'distance', '[', 'moon', 'degrees', 'during', 'km']\n",
      "2015-12-30 23:54:31\n",
      "['â°', 'km', '=', 'miles', 'degrees', '̂°', 'north', 'x', 'â', 'east', 'south', 'per']\n",
      "2017-01-16 01:34:55\n",
      "['flight', 'miles', 'hours', 'm', 'west', 'years', 'southern', 'flights', 'east', 'sydney', 'moved', 'times']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'miles', 'km', '0.0', '+', 'm', 'per', '15', 'hours', 'less', 'minutes', 'feet']\n",
      "2018-04-20 09:03:11\n",
      "['|', 'into', 'back', 'ago', 'up', 'try', 'own', '[', 'let', '+', \"'ve\", 'launch']\n",
      "2018-10-07 05:55:37\n",
      "['m', 'miles', 'degrees', 'feet', 'km', 'east', 'west', 'eye', 'height', 'observer', 'x', 'object']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"infrared\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['i', 'question', 'make', 'thread', 'answer', 'claim', 'he', 'please', 'find', 'own', \"'m\", \"'ve\"]\n",
      "2015-12-30 23:54:31\n",
      "['â°', '=', 'km', 'miles', '̂°', 'â', 'x', '̂', 'degrees', 'm', '2016', '10']\n",
      "2017-01-16 01:34:55\n",
      "['=', 'been', 'moved', 'be', 'ice', 'theory', 'he', 'model', 'earth', 'earthers', 'flat', 'map']\n",
      "2017-11-09 16:57:19\n",
      "['miles', 'years', 'km', 'away', 'south', 'north', '=', 'been', 'hours', 'minutes', 'distance', 'pole']\n",
      "2018-04-20 09:03:11\n",
      "['miles', 'effect', 'degrees', 'sagnac', 'light', 'm', 'theory', 'horizon', 'earth', 'distance', 'model', 'eye']\n",
      "2018-10-07 05:55:37\n",
      "['map', 'earth', 'explanation', 'object', 'surface', 'theory', 'evidence', 'wiki', 'horizon', 'eye', 'question', 'm']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"standpoint\", time_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking only at words in all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected 266        cancer 272           respect 294          inclined 305         universally 311     \n",
      "f 326                invent 351           explore 357          slightest 358        eratosthenes 360    \n",
      "google 362           active 368           > 372                expected 372         witnessed 373       \n",
      "fully 379            freely 381           ] 383                seemingly 391        closed 392          \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "particularly 219     young 238            functions 261        flatness 271         3d 310              \n",
      "explaining 318       universally 322      seemingly 325        super 328            powers 331          \n",
      "dangerous 336        display 356          fully 356            endless 357          giving 357          \n",
      "philosophy 357       became 361           active 362           directed 364         sunsets 369         \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax 227         dropped 302          .. 369               directed 371         flatness 380        \n",
      "remotely 382         functions 385        powers 385           navy 394             -- 401              \n",
      "survey 402           escape 409           > 427                heavily 428          rockets 428         \n",
      "interpreted 429      questioned 430       3d 434               incredibly 439       flag 441            \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "inclined 264         -- 274               practically 296      endless 301          flag 305            \n",
      "reveal 309           stream 313           occurred 314         helped 320           fully 326           \n",
      "continually 330      ordinary 347         feed 375             steps 378            secondly 379        \n",
      "worst 381            immediately 384      ' 391                remotely 394         forever 399         \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "..... 173            empty 251            technically 279      dangerous 284        constantly 313      \n",
      "steps 338            aid 341              ?? 349               fits 351             assumed 362         \n",
      "easiest 365          consistently 366     establish 371        incredible 373       closely 377         \n",
      "fully 377            helped 379           u 379                fantastic 395        wonderful 397       \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in [x for x in changey_words if x[1] in words_in_all_windows][:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected            cancer               respect              f                    slightest           \n",
      "eratosthenes         google               expected             fully                closed              \n",
      "arbitrary            experts              critical             special              ultimately          \n",
      "sunsets              chinese              particularly         popular              r                   \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "particularly         3d                   explaining           super                dangerous           \n",
      "display              fully                giving               became               sunsets             \n",
      "becoming             precisely            deeper               pilots               latter              \n",
      "terrible             similarly            eratosthenes         agreed               standards           \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax             powers               escape               rockets              3d                  \n",
      "incredibly           flag                 giving               click                channel             \n",
      "particularly         key                  stay                 cgi                  super               \n",
      "holds                became               necessarily          generally            effectively         \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "inclined             practically          stream               fully                ordinary            \n",
      "immediately          holds                constantly           typical              powerful            \n",
      "instrument           fits                 errors               required             blind               \n",
      "define               adding               closely              daily                direct              \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "constantly           fits                 assumed              consistently         closely             \n",
      "fully                daily                everyday             stops                speaking            \n",
      "furthermore          finally              leaves               forever              mostly              \n",
      "listed               build                laid                 absolute             equations           \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print_changiest_over_time(changiest_words_per_window, time_models, word_list=words_in_all_windows, min_freq=30, remove_func=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['=', 'i', 'sure', 'am', \"'re\", '1', '[', 'do', ')', 'talking', ']', '2']\n",
      "2015-12-30 23:54:31\n",
      "['°', '=', 'km', 'â', '2016', 'south', 'north', '̂°', 'x', 'east', 'miles', 'size']\n",
      "2017-01-16 01:34:55\n",
      "[\"'ve\", 'he', 'me', 'i', 'did', 'questions', 'answer', 'am', 'tom', 'please', 'question', 'post']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'miles', 'km', '0.0', 'years', '+', 'm', 'my', 'thread', 'minutes', '10', 'topic']\n",
      "2018-04-20 09:03:11\n",
      "['who', \"'m\", 'me', 'thread', 'trying', 'wrong', 'evidence', 'nothing', 'question', 'sure', 'am', 'good']\n",
      "2018-10-07 05:55:37\n",
      "['me', 'science', 'my', 'sure', 'evidence', 'rowbotham', 'wrong', 'your', 'tom', 'who', 'i', 'does']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"particularly\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['do', 'let', \"n't\", 'please', 'you', 'want', 'i', 'what', 'why', 'how', \"'re\", 'did']\n",
      "2015-12-30 23:54:31\n",
      "['â°', '°', 'miles', 'north', 'degrees', 'south', '=', 'pole', 'equator', 'east', 'horizon', '̂°']\n",
      "2017-01-16 01:34:55\n",
      "['=', 'degrees', 'hours', 'north', 'west', 'feet', 'east', 'south', 'm', 'angle', 'southern', 'away']\n",
      "2017-11-09 16:57:19\n",
      "['0.0', \"'ll\", 'did', 'moved', \"'m\", 'does', 'his', 'will', 'want', 'topic', \"'d\", 'my']\n",
      "2018-04-20 09:03:11\n",
      "['|', '[', '/', ']', 'm', '10', 'degrees', '*', '+', 'sagnac', '2', '(']\n",
      "2018-10-07 05:55:37\n",
      "['based', 'who', 'world', 'evidence', 'other', 'thread', 'post', 'been', 'claims', 'topic', 'wiki', 'rowbotham']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"leg\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['i', 'he', 'earthers', '=', 'who', 'been', \"'\", 'people', '\\x94', 'ghosts', 'earth', 'you']\n",
      "2015-12-30 23:54:31\n",
      "['your', 'his', 'an', 'science', 'me', 'their', 'scientific', 'evidence', 'someone', '2016', 'he', '°']\n",
      "2017-01-16 01:34:55\n",
      "['he', 'm', '1', 'tom', 'bishop', 'gravity', 'evidence', 'science', 'acceleration', 'nasa', 'x', 'been']\n",
      "2017-11-09 16:57:19\n",
      "['who', 'flat', 'believe', 'these', 'wrong', 'tom', 'know', 'proof', 'nasa', 'science', 'what', 'exist']\n",
      "2018-04-20 09:03:11\n",
      "[\"'re\", \"'m\", 'thread', 'people', 'who', 'want', 'post', 'questions', 'own', 'please', 'topic', 'am']\n",
      "2018-10-07 05:55:37\n",
      "['mass', 'force', 'degrees', 'm', 'km', 'bodies', 'acceleration', 'miles', 'effect', 'body', 'north', 'has']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"insane\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['am', 'i', 'he', 'claim', 'sure', 'question', 'there', \"'ve\", 'me', 'my', 'evidence', 'answer']\n",
      "2015-12-30 23:54:31\n",
      "['°', '2016', 'km', 'â°', '[', 'â', 'x', ']', '%', '/', '1', 'years']\n",
      "2017-01-16 01:34:55\n",
      "['been', 'he', 'let', 'tom', 'me', 'bishop', 'who', 'i', 'please', 'trying', 'sure', 'answer']\n",
      "2017-11-09 16:57:19\n",
      "[\"'m\", 'tom', 'evidence', 'you', 'i', 'wrong', 'questions', 'me', 'who', 'trying', 'they', 'please']\n",
      "2018-04-20 09:03:11\n",
      "[\"'m\", '=', 'ca', 'do', 'did', 'am', 'me', 'sure', 'let', 'seems', 'tom', 'wo']\n",
      "2018-10-07 05:55:37\n",
      "['miles', 'm', '=', 'degrees', 'km', 'feet', 'x', 'per', '2', '+', 'hours', 'north']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"unreasonable\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['[', ']', '\\x94', \"'\", 'space', 'miles', '/', 'observer', '1', 'x', 'during', 'years']\n",
      "2015-12-30 23:54:31\n",
      "['they', 'those', 'these', 'earthers', 'who', \"'m\", 'you', 'their', 'many', 'scientists', 'nasa', 'science']\n",
      "2017-01-16 01:34:55\n",
      "[\"'re\", 'who', 'are', 'years', 'been', \"'m\", 'questions', 'many', 'am', 'were', \"'ve\", 'topic']\n",
      "2017-11-09 16:57:19\n",
      "[\"'re\", 'who', '=', 'wrong', 'am', \"'\", '0.0', 'trying', 'fake', 'saying', \"'d\", 'talking']\n",
      "2018-04-20 09:03:11\n",
      "['their', 'who', 'these', 'space', 'they', 'those', 'many', 'years', 'things', 'nasa', 'please', 'trying']\n",
      "2018-10-07 05:55:37\n",
      "['people', 'these', '=', 'are', 'years', 'who', 'talking', 'maps', '>', 'nasa', 'space', 'relativity']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"idiots\", time_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at changiest FE Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected 266        eric 351             eratosthenes 360     witnessed 373        freely 381          \n",
      "sunsets 413          firstly 420          eers 423             r 423                physicists 425      \n",
      "assumes 431          presenting 431       surveyors 432        pm 436               galileo 438         \n",
      "confirmation 440     instruments 445      relativistic 445     cavendish 452        inconsistent 457    \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "flatness 271         explaining 318       sunsets 369          deeper 387           pilots 391          \n",
      "blindly 393          rendering 409        foucault 413         barrier 419          demonstrating 424   \n",
      "eratosthenes 428     continually 432      weigh 437            demonstrates 450     reproduce 450       \n",
      "sufficiently 450     insufficient 451     observational 451    testable 451         â« 455             \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax 227         compute 279          flatness 380         navy 394             parameters 420      \n",
      "rockets 428          rendering 430        gravitons 443        barrier 452          erroneous 452       \n",
      "blindly 462          construct 462        faulty 466           cgi 468              publish 472         \n",
      "fallacies 484        noaa 486             foucault 495         ruler 500            scientifically 500  \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "continually 330      trig 343             musk 364             cable 367            outline 381         \n",
      "gauss 396            ruler 428            derive 429           instrument 433       stellar 433         \n",
      "compute 434          firstly 435          errors 437           a. 452               copernicus 461      \n",
      "proposed 468         illustrate 473       reproduce 473        ballistic 475        truths 475          \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "infrared 312         shifting 315         altered 320          seismic 337          convexity 341       \n",
      "gyroscope 369        disproves 398        contention 409       everyday 413         wires 423           \n",
      "microwave 444        expansion 445        kepler 448           equations 453        greeks 455          \n",
      "musk 460             cable 466            thrust 466           gather 469           hypotheses 470      \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in [x for x in changey_words if x[1] in all_kw_list][:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected            eric                 eratosthenes         witnessed            freely              \n",
      "sunsets              firstly              eers                 r                    physicists          \n",
      "assumes              presenting           surveyors            pm                   galileo             \n",
      "confirmation         instruments          relativistic         cavendish            inconsistent        \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "flatness             explaining           sunsets              deeper               pilots              \n",
      "blindly              rendering            foucault             barrier              demonstrating       \n",
      "eratosthenes         continually          weigh                demonstrates         reproduce           \n",
      "sufficiently         insufficient         observational        testable             â«                 \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax             compute              flatness             navy                 parameters          \n",
      "rockets              rendering            gravitons            barrier              erroneous           \n",
      "blindly              construct            faulty               cgi                  publish             \n",
      "fallacies            noaa                 foucault             ruler                scientifically      \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "continually          trig                 musk                 cable                outline             \n",
      "gauss                ruler                derive               instrument           stellar             \n",
      "compute              firstly              errors               a.                   copernicus          \n",
      "proposed             illustrate           reproduce            ballistic            truths              \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "infrared             shifting             altered              seismic              convexity           \n",
      "gyroscope            disproves            contention           everyday             wires               \n",
      "microwave            expansion            kepler               equations            greeks              \n",
      "musk                 cable                thrust               gather               hypotheses          \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print_changiest_over_time(changiest_words_per_window, time_models, word_list=all_kw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "eratosthenes 360     cavendish 452        magnification 489    curving 529          bedford 542         \n",
      "vanishing 544        trigonometry 548     mercator 559         bi-polar 563         auckland 565        \n",
      "diagrams 578         enag 578             azimuthal 580        gps 580              johannesburg 586    \n",
      "equidistant 591      everest 593          santiago 593         ua 595               coriolis 596        \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "eratosthenes 428     diagrams 474         trigonometry 515     bedford 529          everest 538         \n",
      "gps 550              mercator 573         cavendish 579        accelerator 580      balloons 605        \n",
      "auckland 613         enag 616             bi-polar 620         rowbotham 625        coriolis 626        \n",
      "refraction 641       magnification 644    johannesburg 651     ret 656              distances 658       \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "eratosthenes 572     trigonometry 590     cavendish 599        mercator 603         refraction 608      \n",
      "gps 611              bedford 615          enag 620             diagrams 637         bi-polar 639        \n",
      "rowbotham 641        accelerator 642      coriolis 650         magnification 654    balloons 660        \n",
      "auckland 670         ua 671               azimuthal 675        equidistant 681      ret 682             \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "trigonometry 499     eratosthenes 506     spheroid 552         coriolis 553         bedford 558         \n",
      "accelerator 573      diagrams 574         magnification 576    refraction 583       cavendish 589       \n",
      "balloons 602         bi-polar 606         gps 615              antarctic 622        vanishing 633       \n",
      "everest 637          equatorial 641       johannesburg 642     mercator 642         ua 650              \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "enag 530             trigonometry 530     eratosthenes 548     bedford 557          diagrams 564        \n",
      "spheroid 572         gps 594              vanishing 596        bi-polar 602         cavendish 605       \n",
      "accelerator 606      antarctic 629        mercator 629         magnification 630    balloons 631        \n",
      "ret 637              auckland 661         coriolis 665         johannesburg 666     everest 669         \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in [x for x in changey_words if x[1] in t100_kw_list and x[1] in words_in_all_windows][:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "surveyors            cavendish            geodetic             magnification        vanishing           \n",
      "trigonometry         gleason              mercator             azimuthal            gps                 \n",
      "johannesburg         equidistant          santiago             ua                   coriolis            \n",
      "equatorial           ret                  longitude            sunset               rowbotham           \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "diagrams             gps                  qantas               unipolar             balloons            \n",
      "lat                  rowbotham            coriolis             refraction           magnification       \n",
      "ret                  distances            vanishing            santiago             longitude           \n",
      "ua                   fe'ers               antarctic            acceleration         equinox             \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "trigonometry         refraction           gps                  enag                 rowbotham           \n",
      "magnification        balloons             ua                   saros                ret                 \n",
      "distances            vanishing            fe'ers               accelerating         longitude           \n",
      "altitude             tropic               equinox              diagram              acceleration        \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "mirage               coriolis             bedford              theodolite           ligo                \n",
      "diagrams             magnification        refraction           balloons             bi-polar            \n",
      "gps                  vanishing            everest              equatorial           ua                  \n",
      "neutrinos            ret                  converge             rowbotham            curving             \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "enag                 trigonometry         gauss                eratosthenes         diagrams            \n",
      "atmolayer            spheroid             gps                  vanishing            bi-polar            \n",
      "antarctic            mercator             saros                magnification        michelson           \n",
      "ret                  sagnac               coriolis             gyro                 everest             \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print_changiest_over_time(changiest_words_per_window, time_models, word_list=t100_kw_list, min_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['miles', '\\x94', 'observer', 'north', 'during', 'sun', 'distance', 'degrees', 'solar', 'light', 'km', 'between']\n",
      "2015-12-30 23:54:31\n",
      "['°', 'â°', 'km', '2016', 'â', 'x', '̂°', 'miles', 'm', '[', '̂', '/']\n",
      "2017-01-16 01:34:55\n",
      "['round', 'has', \"n't\", 'fe', 'make', 'theory', 'explain', 'globe', 'map', 'not', 'system', 'based']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'horizon', '=', 'light', 'object', 'moon', 'eye', 'miles', 'north', 'speed', 'km', 'observer']\n",
      "2018-04-20 09:03:11\n",
      "['light', 'moon', 'line', 'north', 'angle', 'south', 'surface', 'speed', 'horizon', 'observer', 'earth', 'straight']\n",
      "2018-10-07 05:55:37\n",
      "['m', 'years', '[', 'x', 'â\\x80', 'per', 'he', ']', 'rowbotham', 'sagnac', '+', ':']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"gyroscope\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['sun', 'miles', 'observer', '\\x94', 'light', 'north', 'distance', '[', 'moon', 'degrees', 'during', 'km']\n",
      "2015-12-30 23:54:31\n",
      "['â°', 'km', '=', 'miles', 'degrees', '̂°', 'north', 'x', 'â', 'east', 'south', 'per']\n",
      "2017-01-16 01:34:55\n",
      "['flight', 'miles', 'hours', 'm', 'west', 'years', 'southern', 'flights', 'east', 'sydney', 'moved', 'times']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'miles', 'km', '0.0', '+', 'm', 'per', '15', 'hours', 'less', 'minutes', 'feet']\n",
      "2018-04-20 09:03:11\n",
      "['|', 'into', 'back', 'ago', 'up', 'try', 'own', '[', 'let', '+', \"'ve\", 'launch']\n",
      "2018-10-07 05:55:37\n",
      "['m', 'miles', 'degrees', 'feet', 'km', 'east', 'west', 'eye', 'height', 'observer', 'x', 'object']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"infrared\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['flat', 'claim', 'round', \"n't\", 'there', '\"', 'theory', 'exist', 'what', 'anything', 'wrong', 'why']\n",
      "2015-12-30 23:54:31\n",
      "['they', 'who', 'nasa', 'flat', 'evidence', 'there', 'fake', 'many', 'believe', 'me', 'someone', 'science']\n",
      "2017-01-16 01:34:55\n",
      "['years', 'he', 'who', 'nasa', 'people', 'tom', 'many', 'ago', '=', 'questions', 'done', 'i']\n",
      "2017-11-09 16:57:19\n",
      "['flat', 'theory', 'model', 'gravity', 'globe', 'round', 'conspiracy', 'gravitation', 'force', 'map', \"'\", 'fe']\n",
      "2018-04-20 09:03:11\n",
      "['do', 'nasa', 'who', 'evidence', 'people', 'he', 'they', 'science', 'flat', 'sense', 'round', 'did']\n",
      "2018-10-07 05:55:37\n",
      "['space', 'nasa', 'he', 'who', 'fake', 'science', 'people', 'did', 'wrong', 'his', 'rowbotham', 'done']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"cgi\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42961\n",
      "47076\n",
      "47192\n",
      "47608\n",
      "47627\n",
      "47673\n",
      "47876\n",
      "48290\n",
      "48330\n",
      "48717\n",
      "49640\n",
      "49970\n",
      "49971\n",
      "50233\n",
      "50300\n",
      "50611\n",
      "50711\n",
      "50722\n",
      "50923\n",
      "51170\n",
      "51600\n",
      "51937\n",
      "52084\n",
      "52246\n",
      "52461\n",
      "52952\n",
      "53345\n",
      "53378\n",
      "53922\n",
      "53923\n",
      "54411\n",
      "54440\n",
      "54449\n",
      "55377\n",
      "55461\n",
      "55463\n",
      "55777\n",
      "56094\n",
      "56173\n",
      "56394\n",
      "56423\n",
      "56608\n",
      "56640\n",
      "56666\n",
      "56695\n",
      "56722\n",
      "57078\n",
      "57142\n",
      "57174\n",
      "57194\n",
      "57439\n",
      "57440\n",
      "57549\n",
      "57604\n",
      "57743\n",
      "57852\n",
      "58171\n",
      "58181\n",
      "58574\n",
      "58736\n",
      "58763\n",
      "58917\n",
      "59145\n",
      "59404\n",
      "59428\n",
      "60319\n",
      "60358\n",
      "60479\n",
      "60506\n",
      "60512\n",
      "60860\n",
      "60920\n",
      "79514\n",
      "88968\n",
      "89245\n",
      "92392\n",
      "92682\n",
      "100342\n",
      "112372\n"
     ]
    }
   ],
   "source": [
    "for i, t in toks.items():\n",
    "    if \"â«\" in t:\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_fe_code",
   "language": "python",
   "name": "thesis_fe_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
