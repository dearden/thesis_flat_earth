{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk import ngrams as make_ngrams\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "sys.path.insert(1, \"../\")\n",
    "sys.path.insert(1, \"../utilities\")\n",
    "\n",
    "from helpers import load_posts, load_toks, load_pos, get_top_n_toks\n",
    "from language_change_methods.vnc import VNC, plot_vnc\n",
    "from language_change_methods.utility_functions import get_data_windows, get_time_windows, basic_preprocessing\n",
    "from language_change_methods.features import get_tok_counts, function_words, combine_counts, make_feature_matrix\n",
    "\n",
    "# This method calculates cosine distance between two vectors.\n",
    "from scipy.spatial.distance import cosine as cosine_dist\n",
    "# This method simply inverts it to get similarity.\n",
    "cosine_sim = lambda x,y: 1 - cosine_dist(x,y)\n",
    "\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# suppress some deprecation warning..\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from settings import TFES_FP as DB_FP, TFES_TOK_FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_posts = load_posts(DB_FP)\n",
    "\n",
    "from helpers import flat_earth_boards, off_topic_boards as other_boards\n",
    "\n",
    "fe_posts = all_posts.query(\"board_id in @flat_earth_boards\")\n",
    "\n",
    "toks = {int(x[0]): x[1] for x in load_toks(TFES_TOK_FP)}\n",
    "toks = pd.Series(toks)\n",
    "toks = toks[toks.index.isin(fe_posts.index)]\n",
    "\n",
    "fe_posts = fe_posts.loc[toks.index]\n",
    "fe_posts.sort_values(\"time\", ascending=True)\n",
    "toks = toks.loc[fe_posts.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models Over Time\n",
    "\n",
    "In this section we will train the models on our data. First we'll look at two time periods - the first and second half of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_half = toks.iloc[:int(len(fe_posts)/2)]\n",
    "second_half = toks.iloc[int(len(fe_posts)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_1 = Word2Vec(first_half, size=300)\n",
    "model_2 = Word2Vec(second_half, size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_changey_words_with_models(model1, model2, n=100, k=1000, top_n=None):\n",
    "    nn_scores = []\n",
    "    \n",
    "    top_vocab = sorted(model1.wv.vocab.keys(), key=lambda x: model1.wv.vocab[x].count, reverse=True)[:top_n]\n",
    "    \n",
    "    vocab1 = model1.wv.vocab\n",
    "    vocab2 = model2.wv.vocab\n",
    "    # Loop through all the words in the vocab\n",
    "    for w in vocab1:\n",
    "        if (w not in function_words \n",
    "                and w in vocab1 \n",
    "                and w in vocab2 \n",
    "                and vocab1[w].count > n \n",
    "                and vocab2[w].count > n \n",
    "                and w in top_vocab):\n",
    "            neighbours1 = set([x[0] for x in model1.wv.most_similar(w, topn=k)])\n",
    "            neighbours2 = set([x[0] for x in model2.wv.most_similar(w, topn=k)])\n",
    "            nn_scores.append((len(neighbours1.intersection(neighbours2)), w))\n",
    "            \n",
    "    nn_scores_sorted = sorted(nn_scores)\n",
    "    return nn_scores_sorted\n",
    "\n",
    "\n",
    "\n",
    "def neighbors(query : str,\n",
    "              embs: np.ndarray,\n",
    "              vocab: list,\n",
    "              K : int = 3) -> list:\n",
    "    sims = np.dot(embs[vocab.index(query),],embs.T)\n",
    "    output = []\n",
    "    for sim_idx in sims.argsort()[::-1][1:(1+K)]:\n",
    "        if sims[sim_idx] > 0:\n",
    "            output.append(vocab[sim_idx])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def get_most_changey_words_with_vectors(vocab1, vocab2, vectors1, vectors2, n=20, k=1000):\n",
    "    nn_scores = []\n",
    "    # Loop through all the words in the vocab\n",
    "    for w in vocab1:\n",
    "        if w not in function_words and w in vocab1 and w in vocab2:\n",
    "            neighbours1 = set(neighbors(w, vectors1, vocab1, k))\n",
    "            neighbours2 = set(neighbors(w, vectors2, vocab2, k))\n",
    "            nn_scores.append((len(neighbours1.intersection(neighbours2)), w))\n",
    "            \n",
    "    nn_scores_sorted = sorted(nn_scores)\n",
    "    return nn_scores_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for changiest words from first to second half of corpus\n",
    "\n",
    "We have two subtley different methods for doing this.\n",
    "The first compares the models directly, while the second looks only at the top 10,000 words in the vocabulary.\n",
    "Using the entire models is more \"correct\", but with a small-ish corpus, the second method reduces the effects of low-occurance words and the output makes more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First by comparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranked_words_models = get_most_changey_words_with_models(model_1, model_2, n=10, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(22, 'retarded'),\n",
       " (36, 'conundrum'),\n",
       " (36, 'venture'),\n",
       " (39, 'exaggerated'),\n",
       " (39, 'spoof'),\n",
       " (39, 'whirlpool'),\n",
       " (40, 'fringe'),\n",
       " (41, 'sundial'),\n",
       " (42, 'corona'),\n",
       " (42, 'mouse'),\n",
       " (42, 'raising'),\n",
       " (43, 'joules'),\n",
       " (44, 'bing'),\n",
       " (46, 'expanded'),\n",
       " (46, 'flags'),\n",
       " (46, 'growth'),\n",
       " (46, 'repetition'),\n",
       " (46, 'sighting'),\n",
       " (46, 'trend'),\n",
       " (47, 'dying')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_words_models[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.wv.vocab[\"retarded\"].count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then by comparing the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_vocab_and_vectors(model, n=10000):\n",
    "    \"\"\"\n",
    "    Gets the top n words from the model's vocabulary and the vectors of these words.\n",
    "    \"\"\"\n",
    "    top_vocab = sorted(model.wv.vocab.keys(), key=lambda x: model.wv.vocab[x].count, reverse=True)[:n]\n",
    "    top_vectors = np.array([model.wv[t] for t in top_vocab])\n",
    "    return top_vocab, top_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab_1, vectors_1 = get_top_vocab_and_vectors(model_1)\n",
    "vocab_2, vectors_2 = get_top_vocab_and_vectors(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranked_words_vectors = get_most_changey_words_with_vectors(vocab_1, vocab_2, vectors_1, vectors_2, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(157, 'spoof'),\n",
       " (163, 'intuitively'),\n",
       " (177, 'retarded'),\n",
       " (177, 'whatâ\\x80\\x99s'),\n",
       " (208, 'individually'),\n",
       " (209, 'altogether'),\n",
       " (213, 'endlessly'),\n",
       " (225, 'conundrum'),\n",
       " (225, 'sweeping'),\n",
       " (233, 'beat'),\n",
       " (235, 'replacing'),\n",
       " (236, '3d'),\n",
       " (237, 'imho'),\n",
       " (237, 'unbelievable'),\n",
       " (238, 'qed'),\n",
       " (238, 'rethink'),\n",
       " (242, 'arises'),\n",
       " (242, 'oddly'),\n",
       " (242, 'raising'),\n",
       " (245, 'bone')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_words_vectors[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scientific',\n",
       " 'my',\n",
       " 'google',\n",
       " '3d',\n",
       " 'been',\n",
       " 'i',\n",
       " 'video',\n",
       " ':',\n",
       " 'article',\n",
       " 'your',\n",
       " 'zetetic',\n",
       " 'wiki',\n",
       " 'link',\n",
       " 'posted',\n",
       " 'rowbotham',\n",
       " 'youtube',\n",
       " 'straight',\n",
       " 'new',\n",
       " 'posting',\n",
       " 'provided']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(\"3d\", vectors_1, vocab_1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sphere',\n",
       " 'projection',\n",
       " 'map',\n",
       " 'globe',\n",
       " 'd',\n",
       " 'circle',\n",
       " 'flat',\n",
       " 'coordinate',\n",
       " 'maps',\n",
       " 'plane',\n",
       " 'curved',\n",
       " 'mercator',\n",
       " 'scale',\n",
       " 'bing',\n",
       " 'onto',\n",
       " 'radius',\n",
       " 'lines',\n",
       " '3d',\n",
       " '2d',\n",
       " 'spheroid']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(\"3d\", vectors_2, vocab_2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for more gradual change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time_models = dict()\n",
    "# Train a language model for various different portions of the forum.\n",
    "for w, w_posts in get_data_windows(fe_posts, 10000, 10000):\n",
    "    time_models[w] = Word2Vec(toks.loc[w_posts.index], size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbours_over_time(search_term, time_models, top_n=10000):\n",
    "    for window, curr_model in time_models.items():\n",
    "        curr_vocab, curr_vectors = get_top_vocab_and_vectors(curr_model, top_n)\n",
    "        print(window)\n",
    "        if search_term in curr_vocab:\n",
    "            print(neighbors(search_term, curr_vectors, curr_vocab, 12))\n",
    "            \n",
    "            \n",
    "def neighbours_over_time_comma_delimited(query, time_models, top_n=10000):\n",
    "    for window, curr_model in time_models.items():\n",
    "        curr_vocab, curr_vectors = get_top_vocab_and_vectors(curr_model, top_n)\n",
    "        if query in curr_vocab:\n",
    "            print(window.strftime(\"%Y/%m/%d\"), end=\",\")\n",
    "            curr_neighbours = neighbors(query, curr_vectors, curr_vocab, 12)\n",
    "            print(\",\".join(curr_neighbours[:6]))\n",
    "            print(\"\", end=\",\")\n",
    "            print(\",\".join(curr_neighbours[6:]))\n",
    "        else:\n",
    "            print(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_fe_kw = pd.read_csv(\"../data/top-100-fe-keywords.csv\")\n",
    "t100_kw_list = list(t100_fe_kw[\"ngram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fe_kw = pd.read_csv(\"../data/all-fe-keywords.csv\")\n",
    "all_kw_list = list(all_fe_kw[\"ngram\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at some common FE related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['round', 'globe', 'shape', 'evidence', 'this', 'theory', 'model', 'map', 'i', 'fe', 'believe', 'society']\n",
      "2015-12-30 23:54:31\n",
      "['round', 'globe', 'shape', 'fe', 'evidence', 'sphere', 'map', 'spherical', 'wrong', 'believe', 'true', 'google']\n",
      "2017-01-16 01:34:55\n",
      "['round', 'globe', 'fe', 'shape', 'map', 'evidence', 'model', 'believe', 'real', 'there', 'proof', 'true']\n",
      "2017-11-09 16:57:19\n",
      "['round', 'globe', 'fe', 'shape', 'believe', 'map', 'model', 'spherical', 'theory', 'sphere', 'conspiracy', 'true']\n",
      "2018-04-20 09:03:11\n",
      "['round', 'globe', 'fe', 'map', 'model', 'spherical', 'shape', 'sphere', 'moon', 'theory', 'believe', 'evidence']\n",
      "2018-10-07 05:55:37\n",
      "['round', 'globe', 'fe', 'spherical', 'shape', 'map', 'model', 'sphere', 'believe', 'i', 'moon', 'wrong']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"flat\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013/12/01,round,globe,shape,evidence,this,theory\n",
      ",model,map,i,fe,believe,society\n",
      "2015/12/30,round,globe,shape,fe,evidence,sphere\n",
      ",map,spherical,wrong,believe,true,google\n",
      "2017/01/16,round,globe,fe,shape,map,evidence\n",
      ",model,believe,real,there,proof,true\n",
      "2017/11/09,round,globe,fe,shape,believe,map\n",
      ",model,spherical,theory,sphere,conspiracy,true\n",
      "2018/04/20,round,globe,fe,map,model,spherical\n",
      ",shape,sphere,moon,theory,believe,evidence\n",
      "2018/10/07,round,globe,fe,spherical,shape,map\n",
      ",model,sphere,believe,i,moon,wrong\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time_comma_delimited(\"flat\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['earthers', 'sun', 'horizon', 'model', 'world', 'map', 'surface', 'society', 'theory', 'pole', 'it', 'plane']\n",
      "2015-12-30 23:54:31\n",
      "['horizon', 'earthers', 'sun', 'moon', 'world', 'model', 'map', 'earther', 'surface', 'theory', 'globe', 'object']\n",
      "2017-01-16 01:34:55\n",
      "['world', 'earthers', 'map', 'horizon', 'sun', 'model', 'plane', 'moon', 'surface', 'earther', 'pole', 'theory']\n",
      "2017-11-09 16:57:19\n",
      "['earthers', 'world', 'sun', 'horizon', 'earther', 'model', 'moon', 'map', 'plane', 'surface', 'circle', 'theory']\n",
      "2018-04-20 09:03:11\n",
      "['sun', 'surface', 'moon', 'model', 'world', 'map', 'earthers', 'plane', 'horizon', 'light', 'sphere', 'earther']\n",
      "2018-10-07 05:55:37\n",
      "['surface', 'sun', 'plane', 'earthers', 'map', 'model', 'world', 'moon', 'horizon', 'earther', 'circle', 'maps']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"earth\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['round', 'globe', 'shape', 'theory', 'model', 'evidence', 'map', 'sun', 'earth', 'surface', 'believe', 'curvature']\n",
      "2015-12-30 23:54:31\n",
      "['globe', 'round', 'map', 'shape', 'fe', 'model', 'sphere', 'earth', 'spherical', 'theory', 'evidence', 'moon']\n",
      "2017-01-16 01:34:55\n",
      "['round', 'globe', 'map', 'model', 'shape', 'fe', 'surface', 'earth', 'sphere', 'system', 'plane', 'based']\n",
      "2017-11-09 16:57:19\n",
      "['round', 'globe', 'shape', 'fe', 'model', 'map', 'sphere', 'earth', 'spherical', 'theory', 'plane', 'conspiracy']\n",
      "2018-04-20 09:03:11\n",
      "['round', 'globe', 'model', 'map', 'sphere', 'spherical', 'surface', 'fe', 'moon', 'shape', 'plane', 'theory']\n",
      "2018-10-07 05:55:37\n",
      "['globe', 'round', 'map', 'spherical', 'sphere', 'model', 'surface', 'plane', 'shape', 'fe', 'based', 'maps']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"globe\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['sun', 'flat', 'model', 'round', 'surface', 'horizon', 'globe', 'map', 'observer', 'distance', 'shape', 'pole']\n",
      "2015-12-30 23:54:31\n",
      "['map', 'horizon', 'model', 'sun', 'earthers', 'globe', 'projection', 'surface', 'object', 'earther', 'circle', 'theory']\n",
      "2017-01-16 01:34:55\n",
      "['ice', 'map', 'model', 'surface', 'plane', 'globe', 'wall', 'horizon', 'object', 'force', 'flat', 'circle']\n",
      "2017-11-09 16:57:19\n",
      "['sun', 'horizon', 'surface', 'miles', 'moon', 'circle', 'equator', 'plane', 'object', 'degrees', 'pole', 'north']\n",
      "2018-04-20 09:03:11\n",
      "['surface', 'sun', 'moon', 'plane', 'sphere', 'light', '=', 'distance', 'model', 'miles', 'circle', 'north']\n",
      "2018-10-07 05:55:37\n",
      "['sun', 'north', 'plane', 'map', 'earth', 'pole', 'circle', 'miles', 'equator', 'model', 'south', 'moon']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"disc\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['theory', 'sun', 'evidence', 'no', 'matter', 'acceleration', 'model', 'light', 'aether', 'earth', 'effect', 'exist']\n",
      "2015-12-30 23:54:31\n",
      "['theory', 'evidence', 'sun', 'flat', 'model', 'globe', 'earth', 'does', 'proof', 'fe', 'round', 'shape']\n",
      "2017-01-16 01:34:55\n",
      "['perspective', 'fe', 'acceleration', 'theory', 'evidence', 'model', 'fet', 'anything', 'an', 'flat', 'round', 'tom']\n",
      "2017-11-09 16:57:19\n",
      "['model', 'perspective', 'force', 'acceleration', 'gravitation', 'ua', 'exist', 'shape', 'map', 'theory', 'earth', 'round']\n",
      "2018-04-20 09:03:11\n",
      "['model', 'theory', 'fe', 'evidence', 'effect', 'gravity', 'round', 'perspective', 'coriolis', 'force', 'refraction', 'matter']\n",
      "2018-10-07 05:55:37\n",
      "['force', 'evidence', 'model', 'effect', 'ua', 'fe', 'flat', 'theory', 'acceleration', 'true', 'light', 'round']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"ua\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['earth', 'light', 'observer', 'north', 'surface', 'gravity', 'distance', '=', 'object', 'pole', 'miles', 'moon']\n",
      "2015-12-30 23:54:31\n",
      "['wall', 'pole', 'north', 'horizon', 'object', 'map', '\"', 'celestial', 'surface', 'projection', 'globe', 'lines']\n",
      "2017-01-16 01:34:55\n",
      "['wall', 'pole', 'north', 'force', 'south', 'object', 'acceleration', 'gravity', 'be', 'surface', 'map', 'plane']\n",
      "2017-11-09 16:57:19\n",
      "['wall', 'pole', 'south', 'north', 'object', 'force', 'equator', 'light', 'horizon', 'has', 'earth', 'gravity']\n",
      "2018-04-20 09:03:11\n",
      "['miles', 'flat', 'pole', 'ice', 'map', 'earth', 'round', 'north', 'surface', 'wall', 'globe', '100']\n",
      "2018-10-07 05:55:37\n",
      "['wall', \"'\", 'model', 'pole', 'earth', 'theory', 'evidence', 'body', '=', 'dome', 'north', 'has']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"ice\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['light', 'earth', 'observer', 'distance', 'surface', 'north', 'pole', 'gravity', 'miles', 'horizon', 'moon', 'object']\n",
      "2015-12-30 23:54:31\n",
      "['ice', 'north', 'pole', 'horizon', 'map', '\"', 'south', 'object', 'celestial', 'equator', 'projection', 'surface']\n",
      "2017-01-16 01:34:55\n",
      "['wall', 'pole', 'north', 'south', 'horizon', 'force', 'object', 'map', 'gravity', 'be', 'no', 'equator']\n",
      "2017-11-09 16:57:19\n",
      "['ice', 'pole', 'evidence', 'no', 'south', '\"', 'been', 'object', 'force', 'north', 'gravity', 'shadow']\n",
      "2018-04-20 09:03:11\n",
      "['surface', \"'\", 'map', 'plane', 'pole', 'miles', 'model', 'globe', 'round', 'earth', 'wall', 'there']\n",
      "2018-10-07 05:55:37\n",
      "['ice', \"'\", 'pole', 'evidence', 'there', 'north', 'no', 'model', 'body', 'force', 'problem', 'effect']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"wall\", time_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at some of the top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['observer', '=', 'sun', '\\x94', 'distance', 'north', 'light', 'degrees', 'horizon', 'during', 'south', 'sky']\n",
      "2015-12-30 23:54:31\n",
      "['north', 'km', 'south', '°', '=', 'degrees', 'miles', 'equator', 'east', 'pole', 'angle', '̂°']\n",
      "2017-01-16 01:34:55\n",
      "['line', 'straight', 'north', 'light', 'southern', 'distance', 'south', 'between', 'speed', 'pole', 'northern', 'east']\n",
      "2017-11-09 16:57:19\n",
      "['miles', 'km', 'equator', '=', 'distance', 'between', 'north', 'south', 'line', 'angle', 'lines', 'straight']\n",
      "2018-04-20 09:03:11\n",
      "['south', 'degrees', 'pole', 'between', '=', 'miles', 'distance', 'lines', 'equator', 'east', 'latitude', 'distances']\n",
      "2018-10-07 05:55:37\n",
      "['south', 'equator', 'between', 'latitude', 'degrees', 'line', 'km', 'pole', 'east', 'distance', 'longitude', 'distances']\n",
      "-----------------------------------\n",
      "circumference\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['observer', 'distance', 'miles', 'horizon', 'north', 'light', 'surface', 'moon', 'pole', 'earth', '\\x94', 'south']\n",
      "2015-12-30 23:54:31\n",
      "['miles', 'distance', 'â°', 'north', 'sun', 'equator', 'horizon', '=', 'degrees', 'circumference', 'diameter', 'pole']\n",
      "2017-01-16 01:34:55\n",
      "['north', '=', 'degrees', 'ice', 'south', 'distance', 'feet', 'southern', 'pole', 'surface', 'equator', 'km']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'sun', 'km', 'south', 'north', '=', 'equator', 'surface', 'between', 'distance', 'solar', 'object']\n",
      "2018-04-20 09:03:11\n",
      "['miles', 'sun', 'surface', 'north', 'south', 'degrees', 'rotation', 'moon', 'distance', 'pole', 'km', 'circle']\n",
      "2018-10-07 05:55:37\n",
      "['=', 'm', 'km', 'degrees', 'diameter', 'north', 'surface', 'feet', 'distance', 'equator', 'south', 'rotation']\n",
      "-----------------------------------\n",
      "refraction\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['light', 'observer', 'miles', 'distance', 'horizon', 'north', 'moon', 'speed', '\\x94', '=', 'water', 'pole']\n",
      "2015-12-30 23:54:31\n",
      "['light', 'object', 'horizon', 'gravity', 'angle', 'an', 'force', 'appear', 'lines', 'acceleration', 'distance', 'size']\n",
      "2017-01-16 01:34:55\n",
      "['light', 'lines', 'gravity', 'straight', 'acceleration', 'sun', 'horizon', '\"', 'line', 'effect', 'force', 'moon']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'miles', 'km', 'gravity', 'object', 'light', '2', 'acceleration', 'gravitation', 'force', 'horizon', 'angle']\n",
      "2018-04-20 09:03:11\n",
      "['refraction', 'light', 'force', 'perspective', 'horizon', 'coriolis', 'gravity', 'eye', 'level', 'sagnac', '\"', 'matter']\n",
      "2018-10-07 05:55:37\n",
      "['light', 'force', 'gravity', 'refraction', 'acceleration', 'object', 'horizon', 'mass', 'air', 'pressure', 'atmosphere', 'atmospheric']\n",
      "-----------------------------------\n",
      "mirage\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['=', '\\x94', 'sun', 'miles', 'distance', 'north', 'light', 'horizon', 'moon', 'during', 'object', 'eclipse']\n",
      "2015-12-30 23:54:31\n",
      "['â°', 'km', '°', 'miles', 'object', 'x', 'diameter', '̂°', 'degrees', 'â', 'horizon', '2016']\n",
      "2017-01-16 01:34:55\n",
      "['me', 'miles', 'talking', 'people', 'he', \"'re\", 'tom', 'something', 'anything', 'anyone', \"'m\", 'i']\n",
      "2017-11-09 16:57:19\n",
      "['miles', 'degrees', 'km', '0.0', '+', '3000', 'm', 'object', 'mile', 'gravitation', 'north', 'straight']\n",
      "2018-04-20 09:03:11\n",
      "['light', 'force', 'horizon', 'eye', 'coriolis', 'refraction', 'line', 'level', 'sagnac', 'angle', '\"', '=']\n",
      "2018-10-07 05:55:37\n",
      "['eye', 'light', 'm', 'effect', 'object', 'level', 'height', 'force', 'miles', 'feet', 'acceleration', 'surface']\n",
      "-----------------------------------\n",
      "mercator\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['\\x94', 'observer', 'theory', 'flat', 'globe', 'round', '1', '[', 'map', 'ether', 'model', 'miles']\n",
      "2015-12-30 23:54:31\n",
      "['globe', 'projection', '=', 'ice', 'wall', 'north', 'km', 'pole', 'surface', 'â°', '°', 'circle']\n",
      "2017-01-16 01:34:55\n",
      "['round', 'map', 'model', 'fe', 'globe', 'based', 'earth', 'ice', 'bipolar', 'system', 'acceleration', 'gravity']\n",
      "2017-11-09 16:57:19\n",
      "['force', 'globe', 'gravity', 'flat', 'gravitation', 'celestial', 'miles', 'km', '0.0', 'degrees', 'round', 'solar']\n",
      "2018-04-20 09:03:11\n",
      "['=', 'globe', 'round', 'spherical', 'based', 'sphere', 'surface', 'effect', 'sagnac', 'system', 'map', 'theory']\n",
      "2018-10-07 05:55:37\n",
      "['maps', 'model', 'based', 'system', 'plane', 'globe', 'earth', 'projection', 'spherical', 'round', 'flat', '=']\n",
      "-----------------------------------\n",
      "saros\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['\\x94', 'observer', 'sun', 'miles', 'during', 'north', '[', 'eclipse', 'solar', ']', 'degrees', 'light']\n",
      "2015-12-30 23:54:31\n",
      "2017-01-16 01:34:55\n",
      "['southern', '/', 'm', '1', 'solar', 'northern', 'celestial', 'system', 'km', 'eclipse', 'x', 'acceleration']\n",
      "2017-11-09 16:57:19\n",
      "['years', '=', 'south', 'miles', 'solar', 'hours', 'km', 'southern', 'sun', 'north', 'pole', 'equator']\n",
      "2018-04-20 09:03:11\n",
      "['solar', '=', 'days', 'years', '24', 'year', 'model', 'day', 'based', 'round', 'flat', 'theory']\n",
      "2018-10-07 05:55:37\n",
      "['solar', 'years', '/', 'x', 'm', '[', 'per', '1', '2', '>', 'â\\x80', 'eclipse']\n",
      "-----------------------------------\n",
      "santiago\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['=', 'observer', '\\x94', 'distance', 'north', 'degrees', 'sun', 'km', '[', 'light', 'horizon', 'during']\n",
      "2015-12-30 23:54:31\n",
      "['â°', '°', '=', 'south', 'miles', '2016', 'north', 'sydney', 'east', 'degrees', 'west', 'm']\n",
      "2017-01-16 01:34:55\n",
      "['miles', 'hours', 'degrees', 'north', 'south', 'm', 'feet', 'west', 'sydney', 'east', 'southern', 'km']\n",
      "2017-11-09 16:57:19\n",
      "['miles', '=', 'km', 'hours', 'minutes', 'years', '10', 'feet', 'south', 'm', '+', 'per']\n",
      "2018-04-20 09:03:11\n",
      "['degrees', 'miles', 'south', 'north', 'feet', 'years', 'm', '10', 'hours', '|', 'km', '6']\n",
      "2018-10-07 05:55:37\n",
      "['degrees', 'm', '=', 'km', 'feet', 'hours', 'years', 'per', 'south', 'east', 'west', '15']\n",
      "-----------------------------------\n",
      "equinox\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['sun', 'miles', 'distance', 'horizon', 'north', '\\x94', '=', 'light', 'moon', 'sky', 'during', 'pole']\n",
      "2015-12-30 23:54:31\n",
      "['km', 'north', 'miles', 'equator', 'sun', 'east', 'degrees', '°', 'south', 'horizon', 'angle', 'pole']\n",
      "2017-01-16 01:34:55\n",
      "['horizon', 'north', 'moon', 'equator', 'pole', 'miles', 'southern', 'south', 'angle', 'degrees', 'sunset', 'east']\n",
      "2017-11-09 16:57:19\n",
      "['miles', 'sun', 'km', 'equator', 'horizon', 'moon', 'angle', 'sunset', 'distance', '=', 'north', 'noon']\n",
      "2018-04-20 09:03:11\n",
      "['moon', 'day', 'equinox', 'year', 'solar', 'noon', 'equator', 'degrees', 'observer', 'miles', 'north', 'angle']\n",
      "2018-10-07 05:55:37\n",
      "['moon', 'east', 'equator', 'north', 'day', 'horizon', 'degrees', 'observer', 'west', 'sky', 'shadow', 'miles']\n",
      "-----------------------------------\n",
      "clockwise\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['miles', 'observer', '\\x94', 'sun', 'north', 'distance', 'light', 'degrees', 'km', 'during', '[', 'solar']\n",
      "2015-12-30 23:54:31\n",
      "['north', 'hemisphere', 'â°', 'southern', 'pole', 'east', 'northern', 'equator', 'km', 'sun', 'appear', '°']\n",
      "2017-01-16 01:34:55\n",
      "['south', 'pole', 'southern', 'east', 'northern', 'miles', 'equator', '=', 'west', 'sun', 'hemisphere', 'degrees']\n",
      "2017-11-09 16:57:19\n",
      "['=', 'miles', 'km', 'south', 'north', '10', 'm', 'equator', 'hours', '+', 'per', 'feet']\n",
      "2018-04-20 09:03:11\n",
      "['south', 'degrees', 'north', 'east', 'west', 'miles', 'pole', 'feet', '|', 'm', 'angle', 'km']\n",
      "2018-10-07 05:55:37\n",
      "['north', 'miles', 'west', 'degrees', 'm', 'sun', 'south', '=', 'km', 'horizon', 'feet', 'pole']\n",
      "-----------------------------------\n",
      "trigonometry\n",
      "-----------------------------------\n",
      "2013-12-01 18:43:04\n",
      "['=', '\\x94', 'miles', 'sun', 'distance', 'north', 'horizon', 'earth', '[', 'equator', 'light', 'km']\n",
      "2015-12-30 23:54:31\n",
      "['science', 'scientific', 'theory', 'gravity', 'by', 'own', 'evidence', 'method', 'does', '°', '[', 'an']\n",
      "2017-01-16 01:34:55\n",
      "['moved', 'm', 'x', '/', 'by', '1', 'between', '+', 'flight', \"'ve\", 'system', 'ms']\n",
      "2017-11-09 16:57:19\n",
      "['model', 'gravity', 'these', 'perspective', 'gravitation', 'lines', 'fe', 'system', 'our', 'based', 'laws', 'theory']\n",
      "2018-04-20 09:03:11\n",
      "['|', '[', '/', 'm', 'degrees', 'between', ']', 'distances', 'south', '+', 'â\\x80', 'north']\n",
      "2018-10-07 05:55:37\n",
      "['map', 'earth', 'plane', 'system', 'model', 'distances', 'surface', 'earthers', 'circle', 'based', 'models', 'globe']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for w in t100_kw_list[:10]:\n",
    "    print(w)\n",
    "    print(\"-----------------------------------\")\n",
    "    neighbours_over_time(w, time_models)\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for the changiest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changiest_words_per_window(time_models, top_n=10000):\n",
    "    out_dic = dict()\n",
    "    windows = list(time_models.keys())\n",
    "    for i in range(1, len(windows)):\n",
    "        model_1 = time_models[windows[i-1]]\n",
    "        model_2 = time_models[windows[i]]\n",
    "\n",
    "        vocab_1, vectors_1 = get_top_vocab_and_vectors(model_1, top_n)\n",
    "        vocab_2, vectors_2 = get_top_vocab_and_vectors(model_2, top_n)\n",
    "\n",
    "        out_dic[windows[i]] = get_most_changey_words_with_vectors(vocab_1, vocab_2, vectors_1, vectors_2, k=1000)\n",
    "\n",
    "    return out_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "changiest_words_per_window = get_changiest_words_per_window(time_models, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_lists = lambda x: list(itertools.chain.from_iterable(x))\n",
    "all_words = set(merge_lists([[cw[1] for cw in cws] for cws in changiest_words_per_window.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_all_windows(changiest_words_per_window):\n",
    "    words_in_each_window = [set([cw[1] for cw in cws]) for cws in changiest_words_per_window.values()]\n",
    "    words_in_all_windows = words_in_each_window[0].intersection(*words_in_each_window[1:])\n",
    "    return words_in_all_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_all_windows = get_words_in_all_windows(changiest_words_per_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected 254        cancer 267           respect 294          trade 328            witnessed 331       \n",
      "universally 332      f 333                blah 334             alt 356              quiet 360           \n",
      "invent 361           offers 363           spreading 364        slightest 368        freely 372          \n",
      "> 373                google 377           ] 379                [ 382                closed 385          \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "particularly 203     functions 210        junk 221             3d 248               peoples 255         \n",
      "young 257            ** 276               terrible 278         moderate 306         flatness 307        \n",
      "super 309            directed 322         representative 324   plainly 325          notes 326           \n",
      "heavily 328          paint 332            universally 336      sattelites 338       explaining 339      \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax 224         compute 260          directed 273         dropped 326          super 342           \n",
      "needing 348          functions 355        .. 374               3d 387               alternatively 390   \n",
      "escape 390           expressed 391        dictate 393          interestingly 397    blindly 398         \n",
      "itâs 406          navy 406             pin 408              -- 410               powers 414          \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "drives 236           stream 276           -- 283               continually 285      photoshopped 297    \n",
      "??? 312              selective 312        helped 322           inclined 328         occurred 335        \n",
      "endless 336          implied 339          feed 343             examination 346      shared 346          \n",
      "flag 347             immediately 351      reveal 357           arenât 358        virtually 365       \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "..... 164            empty 264            drives 265           technically 290      letter 306          \n",
      "infrared 307         beat 317             convexity 317        altered 322          disproves 323       \n",
      "fits 335             assess 337           assumed 338          dangerous 343        closely 344         \n",
      "prime 346            seismic 352          constantly 358       aid 360              stood 362           \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in changey_words[:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected            cancer               respect              f                    alt                 \n",
      "slightest            google               closed               expected             experts             \n",
      "ultimately           particularly         fully                junker               watching            \n",
      "direct               existed              new                  includes             non                 \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "particularly         3d                   terrible             super                explaining          \n",
      "became               giving               fully                display              pilots              \n",
      "sunsets              deeper               includes             becoming             necessarily         \n",
      "consistently         precisely            confusion            similarly            non                 \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax             compute              super                3d                   escape              \n",
      "itâs              powers               necessarily          click                particularly        \n",
      "rockets              stay                 channel              rendering            s                  \n",
      "flag                 giving               highly               sat                  foucault            \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "stream               inclined             immediately          club                 fits                \n",
      "fully                derive               define               constantly           adding              \n",
      "advocates            cut                  ordinary             practically          seemingly           \n",
      "cell                 typical              powerful             required             famous              \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "beat                 fits                 assumed              closely              seismic             \n",
      "constantly           finally              furthermore          stops                wires               \n",
      "consistently         literal              speaking             forever              fully               \n",
      "absolute             holds                daily                itâ                eliminate           \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "from word_vector_change import print_changiest_over_time\n",
    "\n",
    "print_changiest_over_time(changiest_words_per_window, time_models, min_freq=30, remove_func=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['sun', 'miles', 'light', 'observer', '\\x94', 'north', 'distance', 'moon', 'degrees', 'eclipse', 'solar', 'during']\n",
      "2015-12-30 23:54:31\n",
      "['=', 'sun', 'miles', 'km', '°', 'pole', 'north', 'diameter', 'object', 'gravity', 'light', 'degrees']\n",
      "2017-01-16 01:34:55\n",
      "['miles', 'degrees', 'speed', 'light', 'due', 'sun', 'feet', 'km', 'west', 'm', 'between', 'away']\n",
      "2017-11-09 16:57:19\n",
      "['tom', 'evidence', 'rowbotham', 'been', 'i', 'anyone', 'thread', 'who', 'science', 'question', 'someone', 'me']\n",
      "2018-04-20 09:03:11\n",
      "['[', 'years', 'theory', ']', '|', 'm', '24', 'been', '10', 'days', 'miles', 'â\\x80']\n",
      "2018-10-07 05:55:37\n",
      "['relativity', '=', 'acceleration', 'sagnac', 'gravity', 'force', 'system', 'coriolis', 'based', 'rowbotham', 'body', 'by']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"parallax\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "[\"'m\", 'am', 'post', \"'ll\", 'thread', 'have', 'want', 'i', 'had', 'read', 'question', \"'d\"]\n",
      "2015-12-30 23:54:31\n",
      "['=', '°', 'â°', '2016', 'miles', 'â', 'years', '10', 'm', 'x', 'degrees', 'per']\n",
      "2017-01-16 01:34:55\n",
      "['m', 'miles', '1', '2', '/', 'x', 'km', '+', 'hours', '*', '%', 'years']\n",
      "2017-11-09 16:57:19\n",
      "['=', 'degrees', 'miles', 'years', 'km', '0.0', '+', 'south', '10', 'm', 'feet', 'north']\n",
      "2018-04-20 09:03:11\n",
      "['years', '10', 'thread', 'post', '[', '|', 'm', 'degrees', ']', '*', 'days', 'hours']\n",
      "2018-10-07 05:55:37\n",
      "['done', 'miles', 'years', 'm', 'he', '=', 'i', 'feet', 'km', 'someone', 'wrong', 'me']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"corrected\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "[\"'m\", \"n't\", 'sure', 'wrong', 'exist', 'talking', 'know', 'saying', ')', 'there', 'not', '\\x94']\n",
      "2015-12-30 23:54:31\n",
      "['sun', 'appear', 'object', 'away', 'light', 'pole', 'moon', 'flat', 'distance', 'lines', 'miles', 'it']\n",
      "2017-01-16 01:34:55\n",
      "['lines', \"'\", 'gravity', 'light', 'ice', 'north', 'acceleration', 'straight', 'miles', 'line', 'pole', 'force']\n",
      "2017-11-09 16:57:19\n",
      "['horizon', 'gravity', 'sun', 'degrees', 'miles', '0.0', 'wrong', 'flat', 'object', 'gravitation', 'eye', 'mean']\n",
      "2018-04-20 09:03:11\n",
      "[\"'m\", 'saying', 'well', 'been', \"'re\", 'are', 'he', 'wrong', 'said', 'good', 'sure', 'pretty']\n",
      "2018-10-07 05:55:37\n",
      "['m', 'miles', 'km', 'x', 'degrees', 'feet', '>', 'per', '+', 'north', '/', 'acceleration']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"technically\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['=', 'i', 'sure', '1', 'am', '[', \"'re\", ')', ']', '2', 'do', 'talking']\n",
      "2015-12-30 23:54:31\n",
      "['â°', '°', 'km', 'â', 'miles', '2016', 'x', '̂°', 'diameter', 'degrees', 'per', '10']\n",
      "2017-01-16 01:34:55\n",
      "[\"'ve\", 'he', 'me', 'did', 'am', 'questions', 'answer', 'tom', 'please', 'does', 'question', 'i']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'miles', 'years', 'km', '0.0', '+', 'm', '10', 'my', 'topic', 'minutes', '1']\n",
      "2018-04-20 09:03:11\n",
      "['tom', 'me', 'who', 'thread', 'evidence', 'wrong', 'trying', 'question', 'am', 'good', 'sure', 'talking']\n",
      "2018-10-07 05:55:37\n",
      "['science', 'me', 'who', 'relativity', 'my', 'evidence', '=', 'tom', 'wrong', 'm', 'scientific', 'sure']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"particularly\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['sun', 'observer', 'miles', '\\x94', 'light', 'north', 'distance', 'moon', '[', 'degrees', 'during', 'km']\n",
      "2015-12-30 23:54:31\n",
      "['°', '=', 'km', 'miles', 'degrees', '̂°', 'x', 'diameter', 'â', 'per', 'north', 'east']\n",
      "2017-01-16 01:34:55\n",
      "['flight', 'hours', 'years', 'm', 'miles', 'west', 'flights', 'moved', 'sydney', 'southern', 'times', 'x']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'miles', 'km', '0.0', '+', 'm', 'hours', '10', 'feet', 'per', '15', 'minutes']\n",
      "2018-04-20 09:03:11\n",
      "['|', 'into', 'ago', 'back', '[', 'launch', 'own', 'â\\x80', 'years', ']', 'up', 'try']\n",
      "2018-10-07 05:55:37\n",
      "['m', 'degrees', 'miles', 'feet', 'km', 'height', 'east', 'x', 'eye', 'west', 'object', 'observer']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"infrared\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['i', 'question', 'thread', 'make', 'claim', 'answer', 'he', 'please', 'own', 'find', \"'m\", 'forum']\n",
      "2015-12-30 23:54:31\n",
      "['=', 'â°', 'km', '2016', 'years', 'â', 'x', '10', '̂°', '̂', 'per', \"'\"]\n",
      "2017-01-16 01:34:55\n",
      "['been', '=', 'moved', 'ice', 'he', 'theory', 'be', 'earth', 'earthers', 'model', 'anyone', 'based']\n",
      "2017-11-09 16:57:19\n",
      "['miles', '=', 'km', 'south', 'north', 'years', 'away', 'pole', '0.0', 'distance', 'hours', '10']\n",
      "2018-04-20 09:03:11\n",
      "['miles', 'degrees', 'm', 'sagnac', 'effect', '10', '[', 'feet', '1', 'light', 'theory', '|']\n",
      "2018-10-07 05:55:37\n",
      "['object', 'eye', 'horizon', 'explanation', 'earth', 'map', 'question', 'm', 'observer', 'surface', 'wiki', 'point']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"standpoint\", time_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking only at words in all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected 254        cancer 267           respect 294          witnessed 331        universally 332     \n",
      "f 333                invent 361           slightest 368        freely 372           > 373               \n",
      "google 377           ] 379                [ 382                closed 385           expected 389        \n",
      "active 395           firstly 405          referred 406         seemingly 409        experts 412         \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "particularly 203     functions 210        3d 248               young 257            terrible 278        \n",
      "flatness 307         super 309            directed 322         heavily 328          universally 336     \n",
      "explaining 339       seemingly 346        solely 347           became 358           entry 362           \n",
      "giving 365           readily 365          fully 375            greatly 378          display 379         \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax 224         directed 273         dropped 326          super 342            functions 355       \n",
      ".. 374               3d 387               escape 390           blindly 398          navy 406            \n",
      "-- 410               powers 414           beautiful 418        flatness 419         necessarily 432     \n",
      "survey 436           ??? 437              heavily 437          questioned 437       empty 440           \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "stream 276           -- 283               continually 285      ??? 312              helped 322          \n",
      "inclined 328         occurred 335         endless 336          feed 343             flag 347            \n",
      "immediately 351      reveal 357           virtually 365        fits 370             fully 372           \n",
      "nicely 378           steps 387            worst 388            beautiful 390        begins 397          \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "..... 164            empty 264            technically 290      fits 335             assumed 338         \n",
      "dangerous 343        closely 344          prime 346            constantly 358       aid 360             \n",
      "stood 362            steps 364            incredible 370       finally 376          ?? 380              \n",
      "establish 386        furthermore 388      stops 390            wonderful 390        helped 392          \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in [x for x in changey_words if x[1] in words_in_all_windows][:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected            cancer               respect              f                    slightest           \n",
      "google               closed               expected             experts              ultimately          \n",
      "particularly         fully                watching             direct               existed             \n",
      "new                  includes             non                  folks                pm                  \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "particularly         3d                   terrible             super                explaining          \n",
      "became               giving               fully                display              pilots              \n",
      "sunsets              deeper               includes             becoming             necessarily         \n",
      "consistently         precisely            confusion            similarly            non                 \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax             super                3d                   escape               powers              \n",
      "necessarily          click                particularly         rockets              stay                \n",
      "channel              s                   flag                 giving               highly              \n",
      "sat                  cgi                  experienced          capable              kept                \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "stream               inclined             immediately          fits                 fully               \n",
      "define               constantly           adding               cut                  ordinary            \n",
      "practically          seemingly            cell                 typical              powerful            \n",
      "required             famous               usually              raise                direct              \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "fits                 assumed              closely              constantly           finally             \n",
      "furthermore          stops                consistently         speaking             forever             \n",
      "fully                absolute             holds                daily                knowing             \n",
      "mostly               build                figures              equations            everyday            \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print_changiest_over_time(changiest_words_per_window, time_models, word_list=words_in_all_windows, min_freq=30, remove_func=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['=', 'i', 'sure', '1', 'am', '[', \"'re\", ')', ']', '2', 'do', 'talking']\n",
      "2015-12-30 23:54:31\n",
      "['â°', '°', 'km', 'â', 'miles', '2016', 'x', '̂°', 'diameter', 'degrees', 'per', '10']\n",
      "2017-01-16 01:34:55\n",
      "[\"'ve\", 'he', 'me', 'did', 'am', 'questions', 'answer', 'tom', 'please', 'does', 'question', 'i']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'miles', 'years', 'km', '0.0', '+', 'm', '10', 'my', 'topic', 'minutes', '1']\n",
      "2018-04-20 09:03:11\n",
      "['tom', 'me', 'who', 'thread', 'evidence', 'wrong', 'trying', 'question', 'am', 'good', 'sure', 'talking']\n",
      "2018-10-07 05:55:37\n",
      "['science', 'me', 'who', 'relativity', 'my', 'evidence', '=', 'tom', 'wrong', 'm', 'scientific', 'sure']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"particularly\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['do', \"n't\", 'please', 'let', 'you', 'want', 'i', 'what', \"'re\", 'claim', 'why', 'how']\n",
      "2015-12-30 23:54:31\n",
      "['°', 'km', '=', 'miles', 'north', '̂°', 'east', 'degrees', 'south', 'â', 'x', 'diameter']\n",
      "2017-01-16 01:34:55\n",
      "['=', 'degrees', 'hours', 'north', 'west', 'feet', 'east', 'south', 'm', 'angle', 'southern', 'away']\n",
      "2017-11-09 16:57:19\n",
      "['able', 'ca', 'want', 'does', 'miles', '0.0', 'degrees', 'will', 'they', 'try', 'need', \"'ll\"]\n",
      "2018-04-20 09:03:11\n",
      "['[', '/', ']', '|', 'm', '10', 'degrees', '2', '(', 'sagnac', '*', '+']\n",
      "2018-10-07 05:55:37\n",
      "['who', 'evidence', 'based', 'claims', 'wiki', 'world', 'thread', 'claim', 'rowbotham', 'nasa', 'post', 'science']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"leg\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['i', 'earthers', 'he', 'who', \"'\", 'people', 'been', 'flat', '=', 'earth', 'you', 'ghosts']\n",
      "2015-12-30 23:54:31\n",
      "['â°', '°', '=', '2016', 'â', 'miles', 'x', '10', '̂°', 'm', '̂', 'degrees']\n",
      "2017-01-16 01:34:55\n",
      "['he', 'tom', 'bishop', '1', 'm', 'evidence', 'science', 'gravity', 'acceleration', 'nasa', 'been', 'god']\n",
      "2017-11-09 16:57:19\n",
      "['flat', '0.0', 'evidence', 'questions', 'many', 'these', 'been', '+', 'round', \"'ve\", '!', \"'re\"]\n",
      "2018-04-20 09:03:11\n",
      "[\"'re\", 'thread', 'who', \"'m\", 'people', 'want', 'please', 'own', 'post', 'questions', 'read', 'am']\n",
      "2018-10-07 05:55:37\n",
      "['mass', 'km', 'miles', 'degrees', 'm', 'force', 'acceleration', 'bodies', 'gravitational', 'feet', 'less', 'north']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"insane\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['he', 'i', 'am', 'claim', 'sure', 'there', 'question', 'me', 'my', \"'ve\", 'you', 'evidence']\n",
      "2015-12-30 23:54:31\n",
      "['am', '=', 'bishop', '[', '2016', 'my', 'let', \"'ll\", '°', 'please', ']', 'post']\n",
      "2017-01-16 01:34:55\n",
      "['he', 'been', 'tom', 'let', 'me', 'bishop', 'who', 'trying', 'please', 'i', 'sure', 'answer']\n",
      "2017-11-09 16:57:19\n",
      "[\"'m\", 'people', 'tom', 'who', 'wrong', 'do', 'does', 'he', 'questions', 'me', 'you', 'proof']\n",
      "2018-04-20 09:03:11\n",
      "[\"'m\", 'ca', 'do', 'did', '=', 'me', 'am', 'let', 'sure', 'tom', 'understand', 'seems']\n",
      "2018-10-07 05:55:37\n",
      "['m', \"'\", '=', 'degrees', 'feet', 'km', 'north', 'x', 'south', 'per', 'pole', 'hours']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"unreasonable\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['[', \"'\", ']', '\\x94', 'miles', 'space', 'observer', '/', '1', 'north', 'km', 'x']\n",
      "2015-12-30 23:54:31\n",
      "['they', 'you', 'me', \"'m\", 'earthers', 'those', 'do', 'want', 'are', 'these', \"'re\", 'who']\n",
      "2017-01-16 01:34:55\n",
      "[\"'re\", 'are', 'who', 'years', 'been', \"'m\", 'questions', 'many', 'am', 'were', \"'ve\", 'topic']\n",
      "2017-11-09 16:57:19\n",
      "[\"'re\", '=', 'trying', 'who', 'talking', 'going', 'wrong', 'people', 'did', 'saying', \"'d\", \"'ll\"]\n",
      "2018-04-20 09:03:11\n",
      "['their', 'who', 'years', 'space', 'these', 'those', 'many', 'they', 'did', 'ca', 'nasa', 'things']\n",
      "2018-10-07 05:55:37\n",
      "['who', 'these', 'many', 'nasa', 'talking', 'years', 'science', 'relativity', 'rowbotham', 'he', '=', 'wrong']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"idiots\", time_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at changiest FE Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected 254        witnessed 331        freely 372           firstly 405          pm 436              \n",
      "sunsets 436          eers 446             surveyors 449        pilots 455           instruments 456     \n",
      "mathematically 456   cavendish 457        compasses 457        distorted 464        faking 464          \n",
      "assumes 466          trigonometry 466     amateur 468          eratosthenes 468     methodology 468     \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "flatness 307         explaining 339       pilots 379           blindly 381          sunsets 390         \n",
      "deeper 392           barrier 407          foucault 409         illustrate 412       testable 418        \n",
      "string 419           everyday 430         reproduce 434        sufficiently 438     demonstrating 443   \n",
      "rendering 443        principle 450        lasers 452           scientifically 455   continually 456     \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax 224         compute 260          blindly 398          navy 406             flatness 419        \n",
      "faulty 424           methodology 448      triangulation 456    publish 457          rockets 458         \n",
      "altered 463          rendering 463        barrier 477          gravitons 478        santa 480           \n",
      "parameters 483       foucault 484         cgi 485              anomaly 491          relates 496         \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "continually 285      trig 380             outline 388          cable 407            derive 416          \n",
      "musk 418             compute 427          paradox 447          corrections 456      illustrate 456      \n",
      "doesnât 462       ge 463               gauss 464            gather 466           discredit 469       \n",
      "unexplained 469      ballistic 470        instrument 472       firstly 473          a. 476              \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "infrared 307         convexity 317        altered 322          disproves 323        seismic 352         \n",
      "wires 397            discredit 424        contention 426       shifting 430         microwave 444       \n",
      "greeks 451           equations 455        everyday 457         gyroscope 463        doesnât 465      \n",
      "demonstrating 468    landings 469         kepler 470           gather 478           tesla 478           \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in [x for x in changey_words if x[1] in all_kw_list][:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "corrected            witnessed            freely               firstly              pm                  \n",
      "sunsets              eers                 surveyors            pilots               instruments         \n",
      "mathematically       cavendish            compasses            distorted            faking              \n",
      "assumes              trigonometry         amateur              eratosthenes         methodology         \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "flatness             explaining           pilots               blindly              sunsets             \n",
      "deeper               barrier              foucault             illustrate           testable            \n",
      "string               everyday             reproduce            sufficiently         demonstrating       \n",
      "rendering            principle            lasers               scientifically       continually         \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "parallax             compute              blindly              navy                 flatness            \n",
      "faulty               methodology          triangulation        publish              rockets             \n",
      "altered              rendering            barrier              gravitons            santa               \n",
      "parameters           foucault             cgi                  anomaly              relates             \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "continually          trig                 outline              cable                derive              \n",
      "musk                 compute              paradox              corrections          illustrate          \n",
      "doesnât           ge                   gauss                gather               discredit           \n",
      "unexplained          ballistic            instrument           firstly              a.                  \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "infrared             convexity            altered              disproves            seismic             \n",
      "wires                discredit            contention           shifting             microwave           \n",
      "greeks               equations            everyday             gyroscope            doesnât          \n",
      "demonstrating        landings             kepler               gather               tesla               \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print_changiest_over_time(changiest_words_per_window, time_models, word_list=all_kw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "cavendish 457        trigonometry 466     eratosthenes 468     magnification 496    curving 513         \n",
      "enag 536             mercator 550         vanishing 550        bedford 556          gps 566             \n",
      "auckland 570         santiago 577         everest 579          azimuthal 587        bi-polar 591        \n",
      "johannesburg 592     balloons 597         diagrams 598         equidistant 600      accelerator 617     \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "trigonometry 485     gps 497              diagrams 504         eratosthenes 523     mercator 568        \n",
      "accelerator 580      bedford 580          cavendish 589        enag 592             bi-polar 609        \n",
      "auckland 610         balloons 623         rowbotham 628        everest 633          azimuthal 639       \n",
      "coriolis 647         distances 647        magnification 653    santiago 654         refraction 659      \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "bedford 551          cavendish 567        eratosthenes 579     trigonometry 580     bi-polar 610        \n",
      "mercator 610         gps 616              enag 617             refraction 617       rowbotham 632       \n",
      "accelerator 636      ua 647               magnification 648    diagrams 650         auckland 660        \n",
      "coriolis 670         distances 670        equidistant 673      ret 675              azimuthal 678       \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "eratosthenes 495     bedford 497          trigonometry 509     accelerator 560      coriolis 564        \n",
      "spheroid 565         cavendish 576        refraction 579       antarctic 605        bi-polar 606        \n",
      "diagrams 607         magnification 607    gps 618              ua 626               balloons 633        \n",
      "everest 633          johannesburg 634     equatorial 646       ret 648              vanishing 651       \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "enag 517             eratosthenes 537     trigonometry 541     diagrams 548         gps 573             \n",
      "bedford 579          spheroid 585         vanishing 590        accelerator 607      antarctic 615       \n",
      "cavendish 616        bi-polar 617         mercator 618         ret 633              balloons 639        \n",
      "magnification 650    coriolis 653         auckland 662         everest 668          johannesburg 673    \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in [x for x in changey_words if x[1] in t100_kw_list and x[1] in words_in_all_windows][:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-30 23:54:31\n",
      "surveyors            cavendish            trigonometry         geodetic             magnification       \n",
      "gleason              mercator             vanishing            gps                  santiago            \n",
      "azimuthal            johannesburg         equidistant          equatorial           converge            \n",
      "longitude            ua                   coriolis             ret                  sunset              \n",
      "-----------------------------\n",
      "2017-01-16 01:34:55\n",
      "gps                  diagrams             unipolar             qantas               lat                 \n",
      "balloons             rowbotham            coriolis             distances            magnification       \n",
      "santiago             refraction           vanishing            fe'ers               longitude           \n",
      "ret                  equinox              ua                   hemiplane            latitudes           \n",
      "-----------------------------\n",
      "2017-11-09 16:57:19\n",
      "trigonometry         gps                  enag                 refraction           rowbotham           \n",
      "ua                   magnification        distances            ret                  saros               \n",
      "vanishing            balloons             accelerating         longitude            fe'ers              \n",
      "altitude             diagram              acceleration         equinox              tropic              \n",
      "-----------------------------\n",
      "2018-04-20 09:03:11\n",
      "bedford              theodolite           coriolis             mirage               ligo                \n",
      "refraction           bi-polar             diagrams             magnification        gps                 \n",
      "ua                   balloons             everest              equatorial           ret                 \n",
      "vanishing            curving              neutrinos            enag                 rowbotham           \n",
      "-----------------------------\n",
      "2018-10-07 05:55:37\n",
      "enag                 eratosthenes         trigonometry         gauss                diagrams            \n",
      "atmolayer            gps                  spheroid             vanishing            antarctic           \n",
      "bi-polar             mercator             ret                  saros                michelson           \n",
      "magnification        gyro                 coriolis             sagnac               everest             \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print_changiest_over_time(changiest_words_per_window, time_models, word_list=t100_kw_list, min_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['miles', '\\x94', 'observer', 'north', 'during', 'sun', 'distance', 'degrees', 'solar', 'km', 'light', 'between']\n",
      "2015-12-30 23:54:31\n",
      "['°', '=', 'km', '2016', '̂°', 'â', 'miles', 'x', '̂', 'diameter', '10', 'north']\n",
      "2017-01-16 01:34:55\n",
      "['round', \"n't\", 'has', 'make', 'explain', 'fe', 'not', 'theory', 'map', 'prove', 'globe', 'work']\n",
      "2017-11-09 16:57:19\n",
      "['=', 'sun', 'miles', 'north', 'km', 'light', 'horizon', 'south', '0.0', 'moon', 'speed', 'object']\n",
      "2018-04-20 09:03:11\n",
      "['light', 'moon', 'north', 'line', 'south', 'surface', 'earth', 'speed', 'observer', 'angle', 'horizon', 'straight']\n",
      "2018-10-07 05:55:37\n",
      "['m', 'x', 'degrees', 'years', 'per', 'miles', 'km', 'feet', '[', '+', '15', 'sagnac']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"gyroscope\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['sun', 'observer', 'miles', '\\x94', 'light', 'north', 'distance', 'moon', '[', 'degrees', 'during', 'km']\n",
      "2015-12-30 23:54:31\n",
      "['°', '=', 'km', 'miles', 'degrees', '̂°', 'x', 'diameter', 'â', 'per', 'north', 'east']\n",
      "2017-01-16 01:34:55\n",
      "['flight', 'hours', 'years', 'm', 'miles', 'west', 'flights', 'moved', 'sydney', 'southern', 'times', 'x']\n",
      "2017-11-09 16:57:19\n",
      "['degrees', 'miles', 'km', '0.0', '+', 'm', 'hours', '10', 'feet', 'per', '15', 'minutes']\n",
      "2018-04-20 09:03:11\n",
      "['|', 'into', 'ago', 'back', '[', 'launch', 'own', 'â\\x80', 'years', ']', 'up', 'try']\n",
      "2018-10-07 05:55:37\n",
      "['m', 'degrees', 'miles', 'feet', 'km', 'height', 'east', 'x', 'eye', 'west', 'object', 'observer']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"infrared\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 18:43:04\n",
      "['flat', 'claim', \"n't\", 'round', 'theory', 'there', 'exist', 'what', '\"', 'why', 'wrong', 'anything']\n",
      "2015-12-30 23:54:31\n",
      "['there', 'who', 'nasa', 'they', 'evidence', 'science', 'he', 'me', 'sense', 'years', 'someone', 'things']\n",
      "2017-01-16 01:34:55\n",
      "['years', 'he', 'who', 'nasa', 'tom', 'people', 'many', '=', 'ago', 'questions', 'done', 'i']\n",
      "2017-11-09 16:57:19\n",
      "['flat', 'model', 'theory', 'globe', 'fe', 'round', 'wall', 'gravity', 'map', 'conspiracy', \"'\", 'force']\n",
      "2018-04-20 09:03:11\n",
      "['do', 'nasa', 'people', 'they', 'who', 'evidence', 'he', 'flat', 'exist', 'round', 'did', 'science']\n",
      "2018-10-07 05:55:37\n",
      "['nasa', 'space', 'he', 'science', 'who', 'people', 'fake', 'rowbotham', 'his', 'someone', 'many', 'wrong']\n"
     ]
    }
   ],
   "source": [
    "neighbours_over_time(\"cgi\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85446\n",
      "92661\n",
      "92810\n",
      "93585\n",
      "93605\n",
      "93666\n",
      "94010\n",
      "94604\n",
      "94653\n",
      "95157\n",
      "96200\n",
      "96615\n",
      "96625\n",
      "97000\n",
      "97109\n",
      "97522\n",
      "97674\n",
      "97686\n",
      "97984\n",
      "98280\n",
      "98881\n",
      "99385\n",
      "99567\n",
      "99770\n",
      "100124\n",
      "100812\n",
      "101304\n",
      "101337\n",
      "102041\n",
      "102042\n",
      "102828\n",
      "102882\n",
      "102895\n",
      "104249\n",
      "104353\n",
      "104355\n",
      "104754\n",
      "105211\n",
      "105297\n",
      "105560\n",
      "105589\n",
      "105790\n",
      "105844\n",
      "105871\n",
      "105908\n",
      "105947\n",
      "106378\n",
      "106443\n",
      "106489\n",
      "106519\n",
      "106832\n",
      "106833\n",
      "106969\n",
      "107040\n",
      "107211\n",
      "107396\n",
      "107912\n",
      "107923\n",
      "108445\n",
      "108661\n",
      "108702\n",
      "108914\n",
      "109350\n",
      "109683\n",
      "109715\n",
      "111060\n",
      "111126\n",
      "111367\n",
      "111402\n",
      "111408\n",
      "111828\n",
      "111895\n",
      "138706\n",
      "152083\n",
      "152600\n",
      "157455\n",
      "157875\n",
      "171462\n",
      "195229\n"
     ]
    }
   ],
   "source": [
    "for i, t in toks.items():\n",
    "    if \"â«\" in t:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think anything I've got here will work, as I need full models rather than keyed vectors.\n",
    "\n",
    "This means I'd probably need to train my own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader\n",
    "\n",
    "# # Download the \"glove-twitter-25\" embeddings\n",
    "# glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# time_models_retrained = dict()\n",
    "# # Train a language model for various different portions of the forum.\n",
    "# for w, w_posts in get_data_windows(fe_posts, 10000, 10000):\n",
    "#     model = None # Need to load a model here\n",
    "#     curr_toks = toks.loc[w_posts.index]\n",
    "#     model.build_vocab(curr_toks, update=True)\n",
    "#     model.train(curr_toks, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "#     time_models_retrained[w] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LanguageChangeApplication",
   "language": "python",
   "name": "languagechangeapplication"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
