{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk import ngrams as make_ngrams\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "sys.path.insert(1, \"../\")\n",
    "sys.path.insert(1, \"../utilities\")\n",
    "\n",
    "from helpers import load_posts, load_toks, load_pos, get_top_n_toks\n",
    "from language_change_methods.vnc import VNC, plot_vnc\n",
    "from language_change_methods.utility_functions import get_data_windows, get_time_windows, basic_preprocessing\n",
    "from language_change_methods.features import get_tok_counts, function_words, combine_counts, make_feature_matrix\n",
    "\n",
    "# This method calculates cosine distance between two vectors.\n",
    "from scipy.spatial.distance import cosine as cosine_dist\n",
    "# This method simply inverts it to get similarity.\n",
    "cosine_sim = lambda x,y: 1 - cosine_dist(x,y)\n",
    "\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# suppress some deprecation warning..\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from settings import TFES_FP as DB_FP, TFES_TOK_FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_posts = load_posts(DB_FP)\n",
    "\n",
    "from helpers import flat_earth_boards, off_topic_boards as other_boards\n",
    "\n",
    "fe_posts = all_posts.query(\"board_id in @flat_earth_boards\")\n",
    "\n",
    "toks = {int(x[0]): x[1] for x in load_toks(TFES_TOK_FP)}\n",
    "toks = pd.Series(toks)\n",
    "toks = toks[toks.index.isin(fe_posts.index)]\n",
    "\n",
    "fe_posts = fe_posts.loc[toks.index]\n",
    "fe_posts.sort_values(\"time\", ascending=True)\n",
    "toks = toks.loc[fe_posts.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models Over Time\n",
    "\n",
    "In this section we will train the models on our data. First we'll look at two time periods - the first and second half of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_half = toks.iloc[:int(len(fe_posts)/2)]\n",
    "second_half = toks.iloc[int(len(fe_posts)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_1 = Word2Vec(first_half, size=300)\n",
    "model_2 = Word2Vec(second_half, size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_changey_words_with_models(model1, model2, n=100, k=1000, top_n=None):\n",
    "    nn_scores = []\n",
    "    \n",
    "    top_vocab = sorted(model1.wv.vocab.keys(), key=lambda x: model1.wv.vocab[x].count, reverse=True)[:top_n]\n",
    "    \n",
    "    vocab1 = model1.wv.vocab\n",
    "    vocab2 = model2.wv.vocab\n",
    "    # Loop through all the words in the vocab\n",
    "    for w in vocab1:\n",
    "        if (w not in function_words \n",
    "                and w in vocab1 \n",
    "                and w in vocab2 \n",
    "                and vocab1[w].count > n \n",
    "                and vocab2[w].count > n \n",
    "                and w in top_vocab):\n",
    "            neighbours1 = set([x[0] for x in model1.wv.most_similar(w, topn=k)])\n",
    "            neighbours2 = set([x[0] for x in model2.wv.most_similar(w, topn=k)])\n",
    "            nn_scores.append((len(neighbours1.intersection(neighbours2)), w))\n",
    "            \n",
    "    nn_scores_sorted = sorted(nn_scores)\n",
    "    return nn_scores_sorted\n",
    "\n",
    "\n",
    "\n",
    "def neighbors(query : str,\n",
    "              embs: np.ndarray,\n",
    "              vocab: list,\n",
    "              K : int = 3) -> list:\n",
    "    sims = np.dot(embs[vocab.index(query),],embs.T)\n",
    "    output = []\n",
    "    for sim_idx in sims.argsort()[::-1][1:(1+K)]:\n",
    "        if sims[sim_idx] > 0:\n",
    "            output.append(vocab[sim_idx])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def get_most_changey_words_with_vectors(vocab1, vocab2, vectors1, vectors2, n=20, k=1000):\n",
    "    nn_scores = []\n",
    "    # Loop through all the words in the vocab\n",
    "    for w in vocab1:\n",
    "        if w not in function_words and w in vocab1 and w in vocab2:\n",
    "            neighbours1 = set(neighbors(w, vectors1, vocab1, k))\n",
    "            neighbours2 = set(neighbors(w, vectors2, vocab2, k))\n",
    "            nn_scores.append((len(neighbours1.intersection(neighbours2)), w))\n",
    "            \n",
    "    nn_scores_sorted = sorted(nn_scores)\n",
    "    return nn_scores_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for changiest words from first to second half of corpus\n",
    "\n",
    "We have two subtley different methods for doing this.\n",
    "The first compares the models directly, while the second looks only at the top 10,000 words in the vocabulary.\n",
    "Using the entire models is more \"correct\", but with a small-ish corpus, the second method reduces the effects of low-occurance words and the output makes more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First by comparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranked_words_models = get_most_changey_words_with_models(model_1, model_2, n=10, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 'retarded'),\n",
       " (32, 'fringe'),\n",
       " (34, 'flags'),\n",
       " (34, 'whirlpool'),\n",
       " (37, 'venture'),\n",
       " (38, 'corona'),\n",
       " (41, 'spoof'),\n",
       " (42, 'inflated'),\n",
       " (43, 'bone'),\n",
       " (43, 'conundrum'),\n",
       " (44, 'growth'),\n",
       " (44, 'raising'),\n",
       " (45, 'fold'),\n",
       " (45, 'joules'),\n",
       " (45, 'repetition'),\n",
       " (46, 'bing'),\n",
       " (46, 'exaggerated'),\n",
       " (46, 'headlight'),\n",
       " (46, 'mouse'),\n",
       " (48, 'cap')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_words_models[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.wv.vocab[\"retarded\"].count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then by comparing the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_vocab_and_vectors(model, n=10000):\n",
    "    \"\"\"\n",
    "    Gets the top n words from the model's vocabulary and the vectors of these words.\n",
    "    \"\"\"\n",
    "    top_vocab = sorted(model.wv.vocab.keys(), key=lambda x: model.wv.vocab[x].count, reverse=True)[:n]\n",
    "    top_vectors = np.array([model.wv[t] for t in top_vocab])\n",
    "    return top_vocab, top_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab_1, vectors_1 = get_top_vocab_and_vectors(model_1)\n",
    "vocab_2, vectors_2 = get_top_vocab_and_vectors(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranked_words_vectors = get_most_changey_words_with_vectors(vocab_1, vocab_2, vectors_1, vectors_2, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(157, 'whataÌ‚\\x80\\x99s'),\n",
       " (164, 'bone'),\n",
       " (165, 'intuitively'),\n",
       " (173, 'spoof'),\n",
       " (177, 'retarded'),\n",
       " (184, 'altogether'),\n",
       " (191, 'sweeping'),\n",
       " (204, 'oddly'),\n",
       " (217, 'fringe'),\n",
       " (222, 'card'),\n",
       " (226, 'trend'),\n",
       " (227, 'raising'),\n",
       " (231, 'imho'),\n",
       " (234, '3d'),\n",
       " (234, 'unbelievable'),\n",
       " (237, 'continuation'),\n",
       " (237, 'individually'),\n",
       " (239, 'qed'),\n",
       " (242, 'cake'),\n",
       " (243, 'arises')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_words_vectors[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'google',\n",
       " '3d',\n",
       " 'i',\n",
       " 'scientific',\n",
       " 'your',\n",
       " ':',\n",
       " 'been',\n",
       " 'video',\n",
       " 'article',\n",
       " 'youtube',\n",
       " 'wiki',\n",
       " 'posted',\n",
       " 'zetetic',\n",
       " 'link',\n",
       " 'edit',\n",
       " 'straight',\n",
       " 'original',\n",
       " 'rowbotham',\n",
       " 'provided']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(\"3d\", vectors_1, vocab_1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sphere',\n",
       " 'projection',\n",
       " 'map',\n",
       " 'd',\n",
       " 'globe',\n",
       " 'circle',\n",
       " 'coordinate',\n",
       " 'flat',\n",
       " 'plane',\n",
       " 'maps',\n",
       " 'curved',\n",
       " 'scale',\n",
       " 'bing',\n",
       " 'mercator',\n",
       " '3d',\n",
       " 'radius',\n",
       " 'onto',\n",
       " '2d',\n",
       " 'spheroid',\n",
       " 'lines']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(\"3d\", vectors_2, vocab_2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for more gradual change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time_models = dict()\n",
    "# Train a language model for various different portions of the forum.\n",
    "for w, w_posts in get_data_windows(fe_posts, 10000, 10000):\n",
    "    time_models[w] = Word2Vec(toks.loc[w_posts.index], size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbours_over_time(search_term, time_models, top_n=10000):\n",
    "    for window, curr_model in time_models.items():\n",
    "        curr_vocab, curr_vectors = get_top_vocab_and_vectors(curr_model, top_n)\n",
    "        print(window)\n",
    "        if search_term in curr_vocab:\n",
    "            print(neighbors(search_term, curr_vectors, curr_vocab, 12))\n",
    "            \n",
    "            \n",
    "def neighbours_over_time_comma_delimited(query, time_models, top_n=10000):\n",
    "    for window, curr_model in time_models.items():\n",
    "        curr_vocab, curr_vectors = get_top_vocab_and_vectors(curr_model, top_n)\n",
    "        if query in curr_vocab:\n",
    "            print(window.strftime(\"%Y/%m/%d\"), end=\",\")\n",
    "            curr_neighbours = neighbors(query, curr_vectors, curr_vocab, 12)\n",
    "            print(\",\".join(curr_neighbours[:6]))\n",
    "            print(\"\", end=\",\")\n",
    "            print(\",\".join(curr_neighbours[6:]))\n",
    "        else:\n",
    "            print(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/top-100-fe-keywords.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-eb3fe9fb979a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt100_fe_kw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../data/top-100-fe-keywords.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mt100_kw_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt100_fe_kw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ngram\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eddie\\documents\\thesis code\\thesis_flat_earth\\env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eddie\\documents\\thesis code\\thesis_flat_earth\\env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eddie\\documents\\thesis code\\thesis_flat_earth\\env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eddie\\documents\\thesis code\\thesis_flat_earth\\env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eddie\\documents\\thesis code\\thesis_flat_earth\\env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/top-100-fe-keywords.csv'"
     ]
    }
   ],
   "source": [
    "t100_fe_kw = pd.read_csv(\"../data/top-100-fe-keywords.csv\")\n",
    "t100_kw_list = list(t100_fe_kw[\"ngram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fe_kw = pd.read_csv(\"../data/all-fe-keywords.csv\")\n",
    "all_kw_list = list(all_fe_kw[\"ngram\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at some common FE related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"flat\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time_comma_delimited(\"flat\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"earth\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"globe\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"disc\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"ua\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"ice\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"wall\", time_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at some of the top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for w in t100_kw_list[:10]:\n",
    "    print(w)\n",
    "    print(\"-----------------------------------\")\n",
    "    neighbours_over_time(w, time_models)\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for the changiest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changiest_words_per_window(time_models, top_n=10000):\n",
    "    out_dic = dict()\n",
    "    windows = list(time_models.keys())\n",
    "    for i in range(1, len(windows)):\n",
    "        model_1 = time_models[windows[i-1]]\n",
    "        model_2 = time_models[windows[i]]\n",
    "\n",
    "        vocab_1, vectors_1 = get_top_vocab_and_vectors(model_1, top_n)\n",
    "        vocab_2, vectors_2 = get_top_vocab_and_vectors(model_2, top_n)\n",
    "\n",
    "        out_dic[windows[i]] = get_most_changey_words_with_vectors(vocab_1, vocab_2, vectors_1, vectors_2, k=1000)\n",
    "\n",
    "    return out_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "changiest_words_per_window = get_changiest_words_per_window(time_models, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_lists = lambda x: list(itertools.chain.from_iterable(x))\n",
    "all_words = set(merge_lists([[cw[1] for cw in cws] for cws in changiest_words_per_window.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_all_windows(changiest_words_per_window):\n",
    "    words_in_each_window = [set([cw[1] for cw in cws]) for cws in changiest_words_per_window.values()]\n",
    "    words_in_all_windows = words_in_each_window[0].intersection(*words_in_each_window[1:])\n",
    "    return words_in_all_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_all_windows = get_words_in_all_windows(changiest_words_per_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in changey_words[:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_vector_change import print_changiest_over_time\n",
    "\n",
    "print_changiest_over_time(changiest_words_per_window, time_models, min_freq=30, remove_func=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"parallax\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"corrected\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neighbours_over_time(\"technically\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"particularly\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"infrared\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"standpoint\", time_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking only at words in all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in [x for x in changey_words if x[1] in words_in_all_windows][:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_changiest_over_time(changiest_words_per_window, time_models, word_list=words_in_all_windows, min_freq=30, remove_func=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"particularly\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"leg\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"insane\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"unreasonable\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"idiots\", time_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at changiest FE Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in [x for x in changey_words if x[1] in all_kw_list][:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_changiest_over_time(changiest_words_per_window, time_models, word_list=all_kw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window, changey_words in changiest_words_per_window.items():\n",
    "    print(window)\n",
    "    t20_words = [f\"{w[1]} {w[0]}\" for w in [x for x in changey_words if x[1] in t100_kw_list and x[1] in words_in_all_windows][:20]]\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[:5]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[5:10]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[10:15]))\n",
    "    print(\"{:20} {:20} {:20} {:20} {:20}\".format(*t20_words[15:20]))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_changiest_over_time(changiest_words_per_window, time_models, word_list=t100_kw_list, min_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"gyroscope\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"infrared\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_over_time(\"cgi\", time_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in toks.items():\n",
    "    if \"aÌ‚Â«\" in t:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think anything I've got here will work, as I need full models rather than keyed vectors.\n",
    "\n",
    "This means I'd probably need to train my own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader\n",
    "\n",
    "# # Download the \"glove-twitter-25\" embeddings\n",
    "# glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# time_models_retrained = dict()\n",
    "# # Train a language model for various different portions of the forum.\n",
    "# for w, w_posts in get_data_windows(fe_posts, 10000, 10000):\n",
    "#     model = None # Need to load a model here\n",
    "#     curr_toks = toks.loc[w_posts.index]\n",
    "#     model.build_vocab(curr_toks, update=True)\n",
    "#     model.train(curr_toks, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "#     time_models_retrained[w] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_fe_code",
   "language": "python",
   "name": "thesis_fe_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
